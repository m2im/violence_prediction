{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d89b670-f565-4457-aef8-3ca49e5813fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:30:31.362830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import evaluate\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e93d6fc-f5a8-4c57-abda-607f9af89687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for models in the hub\n",
    "# m2im/ml-e5-large_finetuned_violence_twitter_all_labels\n",
    "\n",
    "# Options for path_to_model_on_disk\n",
    "# /data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/\n",
    "# /data4/mmendieta/models/xlmt_finetuned_twitter_all_labels/polished-oath-32/epoch_17/\n",
    "# /data4/mmendieta/models/smallLabse_finetuned_twitter_all_labels\n",
    "# /data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels\n",
    "\n",
    "# Options for path_to_tokenized datasets_on_disk\n",
    "# /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results\n",
    "# /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_xlmt_inference_results\n",
    "# /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_labse_inference_results\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"cuda_device\": 3,\n",
    "    \"path_to_model_on_disk\": \"/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/\", \n",
    "    \"model_ckpt\": \"\",\n",
    "    \"max_length\": 32,\n",
    "    \"batch_size\": 1024,\n",
    "    \"dataset_name\": \"/data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_labse_inference_results\",\n",
    "    \"fout\": \"/data3/mmendieta/Violence_data/csv_files_global_scale/labse_inference_test_set_all_labels.csv\"\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea152c",
   "metadata": {},
   "source": [
    "# 1. Instantiate the required pipeline\n",
    "Choose of the three pipelines of interest. The XLM-T pipeline is the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26145bf4",
   "metadata": {},
   "source": [
    "### LaBSE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fafc2b-9979-402a-94a3-ff14109e7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmendieta/transformers/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "violence_pipe = pipeline(\"text-classification\",\n",
    "                         model=args.path_to_model_on_disk, \n",
    "                         device=args.cuda_device,\n",
    "                         framework=\"pt\",\n",
    "                         return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d76bf8",
   "metadata": {},
   "source": [
    "### small-LaBSE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d088c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "violence_pipe = pipeline(model=args.path_to_model_on_disk,\n",
    "                         task=\"text-classification\",\n",
    "                         device=args.cuda_device,\n",
    "                         framework=\"pt\",\n",
    "                         return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a818c",
   "metadata": {},
   "source": [
    "### XLM-T pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43311793",
   "metadata": {},
   "outputs": [],
   "source": [
    "violence_pipe = pipeline(model=args.path_to_model_on_disk,\n",
    "                         task=\"text-classification\",\n",
    "                         device=args.cuda_device,\n",
    "                         framework=\"pt\",\n",
    "                         return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f67815",
   "metadata": {},
   "source": [
    "### E5-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "violence_pipe = pipeline(model=args.path_to_model_on_disk,\n",
    "                         task=\"text-classification\", # This line helps with e5. For the other models is not necessary\n",
    "                         device=args.cuda_device,\n",
    "                         framework=\"pt\",\n",
    "                         return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4414c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pipeline. Use a short text just like you would do in a twitter. \n",
    "# Keep in mind that we are not doing any text preprocessing for this test case.\n",
    "text = \"today is a sunny day\"\n",
    "outputs = violence_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb99b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3e04d",
   "metadata": {},
   "source": [
    "# 2. Use the pipeline to make predictions with the violence dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dbf47",
   "metadata": {},
   "source": [
    "### Load the tokenized test dataset to disk\n",
    "This is the tokenized dataset that includes the columns *text*, *tweetid*, *geo_x*, *geo_y*, and *lang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b375c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok = load_from_disk(args.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6997ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subsmample for testing purposes\n",
    "test_tokenized_ds_sample = ds_tok.shuffle().select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36499097",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_ds_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ad133",
   "metadata": {},
   "source": [
    "### a. Single sample (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_tokenized_ds_sample[10]\n",
    "print(f\"Text: {sample['text']} | Language: {sample['lang']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = violence_pipe(sample['text'])\n",
    "pd.DataFrame(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a2ec6",
   "metadata": {},
   "source": [
    "### b. Multiple sample pipeline (visualizing text, true labels, predictions, and other columns)\n",
    "Make sure to select the apropriate batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For samples only\n",
    "preds = []\n",
    "\n",
    "for i, outputs in enumerate(tqdm(violence_pipe(KeyDataset(test_tokenized_ds_sample, \"text\"), \n",
    "                                               batch_size=args.batch_size,\n",
    "                                               truncation=True),\n",
    "                                 total=len(test_tokenized_ds_sample))):\n",
    "    text = test_tokenized_ds_sample[i]['text']\n",
    "    labels = test_tokenized_ds_sample[i]['labels'].tolist()\n",
    "    tweetid = test_tokenized_ds_sample[i]['tweetid']\n",
    "    lang = test_tokenized_ds_sample[i]['lang']\n",
    "    geo_x = float(test_tokenized_ds_sample[i]['geo_x']) # cast tensor to float\n",
    "    geo_y = float(test_tokenized_ds_sample[i]['geo_y']) # cast tensor to float\n",
    "    preds.append({\n",
    "        'tweetid': tweetid,\n",
    "        'text': text,\n",
    "        'lang': lang,\n",
    "        'geo_x': geo_x,\n",
    "        'geo_y': geo_y,\n",
    "        'labels': labels,\n",
    "        'outputs': outputs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1165d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea070fa0cfb144d5bee98abcbf5e60f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2329158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the entire dataset\n",
    "preds = []\n",
    "\n",
    "for i, outputs in enumerate(tqdm(violence_pipe(KeyDataset(ds_tok, \"text\"), batch_size=args.batch_size,\n",
    "                                              truncation=True),\n",
    "                                 total=len(ds_tok))):\n",
    "    text = ds_tok[i]['text']\n",
    "    labels = ds_tok[i]['labels'].tolist()\n",
    "    tweetid = ds_tok[i]['tweetid']\n",
    "    lang = ds_tok[i]['lang']\n",
    "    geo_x = float(ds_tok[i]['geo_x']) # cast tensor to float\n",
    "    geo_y = float(ds_tok[i]['geo_y']) # cast tensor to float\n",
    "    preds.append({\n",
    "        'tweetid': tweetid,\n",
    "        'text': text,\n",
    "        'lang': lang,\n",
    "        'geo_x': geo_x,\n",
    "        'geo_y': geo_y,\n",
    "        'labels': labels,\n",
    "        'outputs': outputs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bd13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for pred in preds:\n",
    "    row = {\n",
    "        'tweetid': pred['tweetid'],  # Include tweet ID\n",
    "        'text': pred['text'],        # Include text\n",
    "        'lang': pred['lang'],        # Include language\n",
    "        'geo_x': pred['geo_x'],      # Include geo_x coordinate\n",
    "        'geo_y': pred['geo_y'],      # Include geo_y coordinate\n",
    "    }\n",
    "    \n",
    "    # True labels for each column   \n",
    "    row['post1geo10_true'] = pred['labels'][0]\n",
    "    row['post1geo20_true'] = pred['labels'][1]\n",
    "    row['post1geo30_true'] = pred['labels'][2]\n",
    "    row['post1geo50_true'] = pred['labels'][3]\n",
    "    row['post1geo70_true'] = pred['labels'][4]\n",
    "    row['post2geo10_true'] = pred['labels'][5]\n",
    "    row['post2geo20_true'] = pred['labels'][6]\n",
    "    row['post2geo30_true'] = pred['labels'][7]\n",
    "    row['post2geo50_true'] = pred['labels'][8]\n",
    "    row['post2geo70_true'] = pred['labels'][9]\n",
    "    row['post3geo10_true'] = pred['labels'][10]\n",
    "    row['post3geo20_true'] = pred['labels'][11]\n",
    "    row['post3geo30_true'] = pred['labels'][12]\n",
    "    row['post3geo50_true'] = pred['labels'][13]\n",
    "    row['post3geo70_true'] = pred['labels'][14]\n",
    "    row['post7geo10_true'] = pred['labels'][15]\n",
    "    row['post7geo20_true'] = pred['labels'][16]\n",
    "    row['post7geo30_true'] = pred['labels'][17]\n",
    "    row['post7geo50_true'] = pred['labels'][18]\n",
    "    row['post7geo70_true'] = pred['labels'][19]\n",
    "    row['pre1geo10_true'] = pred['labels'][20]\n",
    "    row['pre1geo20_true'] = pred['labels'][21]\n",
    "    row['pre1geo30_true'] = pred['labels'][22]\n",
    "    row['pre1geo50_true'] = pred['labels'][23]\n",
    "    row['pre1geo70_true'] = pred['labels'][24]\n",
    "    row['pre2geo10_true'] = pred['labels'][25]\n",
    "    row['pre2geo20_true'] = pred['labels'][26]\n",
    "    row['pre2geo30_true'] = pred['labels'][27]\n",
    "    row['pre2geo50_true'] = pred['labels'][28]\n",
    "    row['pre2geo70_true'] = pred['labels'][29]\n",
    "    row['pre3geo10_true'] = pred['labels'][30]\n",
    "    row['pre3geo20_true'] = pred['labels'][31]\n",
    "    row['pre3geo30_true'] = pred['labels'][32]\n",
    "    row['pre3geo50_true'] = pred['labels'][33]\n",
    "    row['pre3geo70_true'] = pred['labels'][34]\n",
    "    row['pre7geo10_true'] = pred['labels'][35]\n",
    "    row['pre7geo20_true'] = pred['labels'][36]\n",
    "    row['pre7geo30_true'] = pred['labels'][37]\n",
    "    row['pre7geo50_true'] = pred['labels'][38]\n",
    "    row['pre7geo70_true'] = pred['labels'][39]\n",
    "    \n",
    "    # Predicted scores for each column   \n",
    "    row['post1geo10'] = pred['outputs'][0]['score']\n",
    "    row['post1geo20'] = pred['outputs'][1]['score']\n",
    "    row['post1geo30'] = pred['outputs'][2]['score']\n",
    "    row['post1geo50'] = pred['outputs'][3]['score']\n",
    "    row['post1geo70'] = pred['outputs'][4]['score']\n",
    "    row['post2geo10'] = pred['outputs'][5]['score']\n",
    "    row['post2geo20'] = pred['outputs'][6]['score']\n",
    "    row['post2geo30'] = pred['outputs'][7]['score']\n",
    "    row['post2geo50'] = pred['outputs'][8]['score']\n",
    "    row['post2geo70'] = pred['outputs'][9]['score']\n",
    "    row['post3geo10'] = pred['outputs'][10]['score']\n",
    "    row['post3geo20'] = pred['outputs'][11]['score']\n",
    "    row['post3geo30'] = pred['outputs'][12]['score']\n",
    "    row['post3geo50'] = pred['outputs'][13]['score']\n",
    "    row['post3geo70'] = pred['outputs'][14]['score']\n",
    "    row['post7geo10'] = pred['outputs'][15]['score']\n",
    "    row['post7geo20'] = pred['outputs'][16]['score']\n",
    "    row['post7geo30'] = pred['outputs'][17]['score']\n",
    "    row['post7geo50'] = pred['outputs'][18]['score']\n",
    "    row['post7geo70'] = pred['outputs'][19]['score']\n",
    "    row['pre1geo10'] = pred['outputs'][20]['score']\n",
    "    row['pre1geo20'] = pred['outputs'][21]['score']\n",
    "    row['pre1geo30'] = pred['outputs'][22]['score']\n",
    "    row['pre1geo50'] = pred['outputs'][23]['score']\n",
    "    row['pre1geo70'] = pred['outputs'][24]['score']\n",
    "    row['pre2geo10'] = pred['outputs'][25]['score']\n",
    "    row['pre2geo20'] = pred['outputs'][26]['score']\n",
    "    row['pre2geo30'] = pred['outputs'][27]['score']\n",
    "    row['pre2geo50'] = pred['outputs'][28]['score']\n",
    "    row['pre2geo70'] = pred['outputs'][29]['score']\n",
    "    row['pre3geo10'] = pred['outputs'][30]['score']\n",
    "    row['pre3geo20'] = pred['outputs'][31]['score']\n",
    "    row['pre3geo30'] = pred['outputs'][32]['score']\n",
    "    row['pre3geo50'] = pred['outputs'][33]['score']\n",
    "    row['pre3geo70'] = pred['outputs'][34]['score']\n",
    "    row['pre7geo10'] = pred['outputs'][35]['score']\n",
    "    row['pre7geo20'] = pred['outputs'][36]['score']\n",
    "    row['pre7geo30'] = pred['outputs'][37]['score']\n",
    "    row['pre7geo50'] = pred['outputs'][38]['score']\n",
    "    row['pre7geo70'] = pred['outputs'][39]['score']\n",
    "    \n",
    "    processed_data.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9442ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns for the specified order of all labels and their predictions\n",
    "df = df[['tweetid', 'text', 'lang', 'geo_x', 'geo_y',\n",
    "         # All 'true' label columns\n",
    "         'post1geo10_true', 'post1geo20_true', 'post1geo30_true', 'post1geo50_true', 'post1geo70_true',\n",
    "         'post2geo10_true', 'post2geo20_true', 'post2geo30_true', 'post2geo50_true', 'post2geo70_true',\n",
    "         'post3geo10_true', 'post3geo20_true', 'post3geo30_true', 'post3geo50_true', 'post3geo70_true',\n",
    "         'post7geo10_true', 'post7geo20_true', 'post7geo30_true', 'post7geo50_true', 'post7geo70_true',\n",
    "         'pre1geo10_true', 'pre1geo20_true', 'pre1geo30_true', 'pre1geo50_true', 'pre1geo70_true',\n",
    "         'pre2geo10_true', 'pre2geo20_true', 'pre2geo30_true', 'pre2geo50_true', 'pre2geo70_true',\n",
    "         'pre3geo10_true', 'pre3geo20_true', 'pre3geo30_true', 'pre3geo50_true', 'pre3geo70_true',\n",
    "         'pre7geo10_true', 'pre7geo20_true', 'pre7geo30_true', 'pre7geo50_true', 'pre7geo70_true',\n",
    "         # All prediction score columns\n",
    "         'post1geo10', 'post1geo20', 'post1geo30', 'post1geo50', 'post1geo70',\n",
    "         'post2geo10', 'post2geo20', 'post2geo30', 'post2geo50', 'post2geo70',\n",
    "         'post3geo10', 'post3geo20', 'post3geo30', 'post3geo50', 'post3geo70',\n",
    "         'post7geo10', 'post7geo20', 'post7geo30', 'post7geo50', 'post7geo70',\n",
    "         'pre1geo10', 'pre1geo20', 'pre1geo30', 'pre1geo50', 'pre1geo70',\n",
    "         'pre2geo10', 'pre2geo20', 'pre2geo30', 'pre2geo50', 'pre2geo70',\n",
    "         'pre3geo10', 'pre3geo20', 'pre3geo30', 'pre3geo50', 'pre3geo70',\n",
    "         'pre7geo10', 'pre7geo20', 'pre7geo30', 'pre7geo50', 'pre7geo70']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878477a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>geo_x</th>\n",
       "      <th>geo_y</th>\n",
       "      <th>post1geo10_true</th>\n",
       "      <th>post1geo20_true</th>\n",
       "      <th>post1geo30_true</th>\n",
       "      <th>post1geo50_true</th>\n",
       "      <th>post1geo70_true</th>\n",
       "      <th>...</th>\n",
       "      <th>pre3geo10</th>\n",
       "      <th>pre3geo20</th>\n",
       "      <th>pre3geo30</th>\n",
       "      <th>pre3geo50</th>\n",
       "      <th>pre3geo70</th>\n",
       "      <th>pre7geo10</th>\n",
       "      <th>pre7geo20</th>\n",
       "      <th>pre7geo30</th>\n",
       "      <th>pre7geo50</th>\n",
       "      <th>pre7geo70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>487300699391524864</td>\n",
       "      <td>#Baloncesto | El seguro ha retrasado la incorp...</td>\n",
       "      <td>es</td>\n",
       "      <td>-66.879189</td>\n",
       "      <td>10.488010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362646</td>\n",
       "      <td>0.341025</td>\n",
       "      <td>0.500007</td>\n",
       "      <td>0.495193</td>\n",
       "      <td>0.456237</td>\n",
       "      <td>0.433460</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.507995</td>\n",
       "      <td>0.503377</td>\n",
       "      <td>0.447357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>447660809615716352</td>\n",
       "      <td>？あなたの質問はモンゴル語ができるかどうかは、関係ないと思いますが。</td>\n",
       "      <td>ja</td>\n",
       "      <td>34.333328</td>\n",
       "      <td>31.416670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453739</td>\n",
       "      <td>0.509758</td>\n",
       "      <td>0.543539</td>\n",
       "      <td>0.465144</td>\n",
       "      <td>0.448671</td>\n",
       "      <td>0.380959</td>\n",
       "      <td>0.414354</td>\n",
       "      <td>0.475345</td>\n",
       "      <td>0.369792</td>\n",
       "      <td>0.367156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>490608380818776065</td>\n",
       "      <td>الحمدلله</td>\n",
       "      <td>ar</td>\n",
       "      <td>35.203289</td>\n",
       "      <td>31.921570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574156</td>\n",
       "      <td>0.606797</td>\n",
       "      <td>0.644596</td>\n",
       "      <td>0.607871</td>\n",
       "      <td>0.626594</td>\n",
       "      <td>0.521864</td>\n",
       "      <td>0.572039</td>\n",
       "      <td>0.611882</td>\n",
       "      <td>0.552581</td>\n",
       "      <td>0.566595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>481456072243552256</td>\n",
       "      <td>Ben hocaları ders konuşan resmi insanlar sanır...</td>\n",
       "      <td>tr</td>\n",
       "      <td>36.567219</td>\n",
       "      <td>36.269169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>0.020455</td>\n",
       "      <td>0.167665</td>\n",
       "      <td>0.624175</td>\n",
       "      <td>0.878551</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.124957</td>\n",
       "      <td>0.469324</td>\n",
       "      <td>0.837316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>476767010660294657</td>\n",
       "      <td>#11J</td>\n",
       "      <td>und</td>\n",
       "      <td>-66.879189</td>\n",
       "      <td>10.488010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>0.017566</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.961599</td>\n",
       "      <td>0.963988</td>\n",
       "      <td>0.962590</td>\n",
       "      <td>0.946629</td>\n",
       "      <td>0.916835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetid                                               text  \\\n",
       "60  487300699391524864  #Baloncesto | El seguro ha retrasado la incorp...   \n",
       "61  447660809615716352                 ？あなたの質問はモンゴル語ができるかどうかは、関係ないと思いますが。   \n",
       "62  490608380818776065                                           الحمدلله   \n",
       "63  481456072243552256  Ben hocaları ders konuşan resmi insanlar sanır...   \n",
       "64  476767010660294657                                               #11J   \n",
       "\n",
       "   lang      geo_x      geo_y  post1geo10_true  post1geo20_true  \\\n",
       "60   es -66.879189  10.488010              0.0              0.0   \n",
       "61   ja  34.333328  31.416670              0.0              0.0   \n",
       "62   ar  35.203289  31.921570              0.0              0.0   \n",
       "63   tr  36.567219  36.269169              0.0              0.0   \n",
       "64  und -66.879189  10.488010              0.0              0.0   \n",
       "\n",
       "    post1geo30_true  post1geo50_true  post1geo70_true  ...  pre3geo10  \\\n",
       "60              0.0              0.0              0.0  ...   0.362646   \n",
       "61              0.0              0.0              0.0  ...   0.453739   \n",
       "62              0.0              0.0              0.0  ...   0.574156   \n",
       "63              0.0              1.0              1.0  ...   0.005823   \n",
       "64              0.0              0.0              0.0  ...   0.020096   \n",
       "\n",
       "    pre3geo20  pre3geo30  pre3geo50  pre3geo70  pre7geo10  pre7geo20  \\\n",
       "60   0.341025   0.500007   0.495193   0.456237   0.433460   0.391691   \n",
       "61   0.509758   0.543539   0.465144   0.448671   0.380959   0.414354   \n",
       "62   0.606797   0.644596   0.607871   0.626594   0.521864   0.572039   \n",
       "63   0.020455   0.167665   0.624175   0.878551   0.006803   0.018300   \n",
       "64   0.017566   0.014054   0.011244   0.010837   0.961599   0.963988   \n",
       "\n",
       "    pre7geo30  pre7geo50  pre7geo70  \n",
       "60   0.507995   0.503377   0.447357  \n",
       "61   0.475345   0.369792   0.367156  \n",
       "62   0.611882   0.552581   0.566595  \n",
       "63   0.124957   0.469324   0.837316  \n",
       "64   0.962590   0.946629   0.916835  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d603737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to disk\n",
    "df.to_csv(args.fout, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565d9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
