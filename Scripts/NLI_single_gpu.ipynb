{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff0b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 15:24:40.770933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from datasets import load_from_disk, Dataset, Sequence, Value\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14996586",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['post1geo10', 'post1geo20', 'post1geo30', 'post1geo50', 'post1geo70', 'post2geo10', 'post2geo20', \n",
    "          'post2geo30', 'post2geo50', 'post2geo70', 'post3geo10', 'post3geo20', 'post3geo30', 'post3geo50', \n",
    "          'post3geo70', 'post7geo10', 'post7geo20', 'post7geo30', 'post7geo50', 'post7geo70', 'pre1geo10', \n",
    "          'pre1geo20', 'pre1geo30', 'pre1geo50', 'pre1geo70', 'pre2geo10', 'pre2geo20', 'pre2geo30', \n",
    "          'pre2geo50', 'pre2geo70', 'pre3geo10', 'pre3geo20', 'pre3geo30', 'pre3geo50', 'pre3geo70', \n",
    "          'pre7geo10', 'pre7geo20', 'pre7geo30', 'pre7geo50', 'pre7geo70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193d55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically extract label names from dataset and create mappings\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6803cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs detected: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220afb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cuda_device\": 14, # Make sure this device is available, else use -1 for CPU\n",
    "    # \"path_to_model_on_disk\": \"/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels/\", # This seems to be your previous model, not the NLI one\n",
    "    \"model_ckpt\": \"mjwong/multilingual-e5-large-xnli\", # The xnli model\n",
    "    \"max_length\": 32,\n",
    "    \"dataset_name\": \"/data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results\", # This is your saved dataset path\n",
    "    \"batch_size\": 1024,\n",
    "    \"fout_nli_csv\": \"/data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results_nli_multilabel_newCategories.csv\"\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59be72",
   "metadata": {},
   "source": [
    "## 4.1 Load the saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0521ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results\n",
      "Dataset loaded. Number of examples: 2329158\n",
      "Features: {'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None)}\n",
      "Dataset after filtering: 2276118 examples (Removed 53040 empty/whitespace texts).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset from: {args.dataset_name}\")\n",
    "try:\n",
    "    ds_with_predictions = load_from_disk(args.dataset_name)\n",
    "    print(f\"Dataset loaded. Number of examples: {len(ds_with_predictions)}\")\n",
    "    print(f\"Features: {ds_with_predictions.features}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load dataset from {args.dataset_name}. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- NEW: Filter out empty or whitespace-only texts ---\n",
    "original_num_examples = len(ds_with_predictions)\n",
    "ds_with_predictions = ds_with_predictions.filter(\n",
    "    lambda example: example['text'] is not None and len(example['text'].strip()) > 0,\n",
    "    desc=\"Filtering out empty or whitespace texts\"\n",
    ")\n",
    "filtered_num_examples = len(ds_with_predictions)\n",
    "print(f\"Dataset after filtering: {filtered_num_examples} examples (Removed {original_num_examples - filtered_num_examples} empty/whitespace texts).\")\n",
    "\n",
    "if filtered_num_examples == 0:\n",
    "    print(\"ERROR: Dataset is empty after filtering! Cannot proceed with NLI.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53052194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text from filtered dataset: 'talking abt my case ☺️' (Type: <class 'str'>)\n"
     ]
    }
   ],
   "source": [
    "# Verify a sample text after filtering\n",
    "if len(ds_with_predictions) > 0:\n",
    "    sample_text = ds_with_predictions[0]['text']\n",
    "    print(f\"Sample text from filtered dataset: '{sample_text}' (Type: {type(sample_text)})\")\n",
    "    if not isinstance(sample_text, str) or len(sample_text.strip()) == 0:\n",
    "        print(\"CRITICAL WARNING: Filtering failed, first example's text is still empty or not a string.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675d17a",
   "metadata": {},
   "source": [
    "## 4.2 Define NLI Hypotheses and Instantiate the NLI Pipeline inside the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c40add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The six hypothesis classes (emotions)\n",
    "nli_hypotheses = [\"fear\", \"anger\", \"joy\", \"hostility\", \"hate\", \"love\", \"disgust\", \"sadness\", \"surprise\", \"trust\", \"anticipation\", \"grief\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3be11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI pipeline loaded on device: cuda:14\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the NLI pipeline (using zero-shot-classification task for NLI)\n",
    "nli_pipe = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=args.model_ckpt,\n",
    "    device=args.cuda_device, \n",
    "    framework=\"pt\",\n",
    "    batch_size=args.batch_size, # Use a batch size that fits your GPU memory\n",
    "    # You might want to set multi_label=True if a tweet can express multiple emotions,\n",
    "    # though for NLI entailment scores, it often handles each hypothesis independently.\n",
    "    # The default for zero-shot is multi_label=False (scores sum to 1 over candidates)\n",
    "    # but for NLI it can also be True (scores are independent). Let's start with default.\n",
    ")\n",
    "\n",
    "print(f\"NLI pipeline loaded on device: {nli_pipe.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cbf47",
   "metadata": {},
   "source": [
    "## 4.3 Define the Function to Add NLI Scores (Batched) \n",
    "\n",
    "This function will take a batch of texts, run the NLI pipeline on them, and return the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec2d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nli_scores_to_example_batched(examples):\n",
    "    \n",
    "    texts = examples['text'] # Get the list of texts for the current batch\n",
    "\n",
    "    # Defensive check (though filter should largely prevent this for 'texts')\n",
    "    # If, for some reason, 'texts' in a batch still contains None or empty strings,\n",
    "    # the pipeline might still struggle. This converts them to a placeholder.\n",
    "    # However, filtering *before* this function is the better approach.\n",
    "    cleaned_texts = [t.strip() if isinstance(t, str) and t.strip() else \"[EMPTY_TEXT]\" for t in texts]\n",
    "    \n",
    "    # --- IMPORTANT: Check for empty batches after cleaning ---\n",
    "    # If all texts in a batch became empty, the pipeline would still fail.\n",
    "    # Return a placeholder for these.\n",
    "    if not cleaned_texts or all(t == \"[EMPTY_TEXT]\" for t in cleaned_texts):\n",
    "        # Create dummy scores for this batch to match the expected output structure\n",
    "        num_examples_in_batch = len(texts)\n",
    "        nli_scores_data = {f\"nli_{hypothesis}\": [0.0] * num_examples_in_batch for hypothesis in nli_hypotheses}\n",
    "        return nli_scores_data\n",
    "    \n",
    "    # Run the NLI pipeline on the batch of texts\n",
    "    # For zero-shot-classification, input is text and candidate_labels\n",
    "    nli_results_batch = nli_pipe(\n",
    "        cleaned_texts,\n",
    "        candidate_labels=nli_hypotheses,\n",
    "        multi_label=True # Consider this if you want independent probabilities per emotion\n",
    "    )\n",
    "\n",
    "    # Initialize lists to store scores for each hypothesis for the current batch\n",
    "    nli_scores_data = {f\"nli_{hypothesis}\": [] for hypothesis in nli_hypotheses}\n",
    "\n",
    "    # Process results for each example in the batch\n",
    "    for result_for_one_text in nli_results_batch:\n",
    "        # result_for_one_text will be a dict like:\n",
    "        # {'sequence': '...', 'labels': ['fear', 'anger', ...], 'scores': [0.9, 0.1, ...]}\n",
    "        \n",
    "        # Create a dictionary for quick lookup of scores by label for this specific text\n",
    "        scores_map = {label: score for label, score in zip(result_for_one_text['labels'], result_for_one_text['scores'])}\n",
    "\n",
    "        # Append the score for each hypothesis to its respective list\n",
    "        for hypothesis in nli_hypotheses:\n",
    "            # We are interested in the entailment score for each hypothesis.\n",
    "            # The zero-shot-classification pipeline internally maps each candidate_label\n",
    "            # to one of the NLI outcomes (entailment, neutral, contradiction)\n",
    "            # and then typically takes the entailment score as the score for that label.\n",
    "            # The 'labels' and 'scores' fields in result_for_one_text are already\n",
    "            # sorted according to candidate_labels.\n",
    "            score = scores_map.get(hypothesis, 0.0) # Get the score, default to 0.0 if not found\n",
    "            nli_scores_data[f\"nli_{hypothesis}\"].append(float(score))\n",
    "\n",
    "    return nli_scores_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fe9db",
   "metadata": {},
   "source": [
    "## 4.4 Update Dataset Features and Apply the Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8abc8af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defined final features for dataset after adding NLI scores:\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None), 'nli_fear': Value(dtype='float32', id=None), 'nli_anger': Value(dtype='float32', id=None), 'nli_joy': Value(dtype='float32', id=None), 'nli_hostility': Value(dtype='float32', id=None), 'nli_hate': Value(dtype='float32', id=None), 'nli_love': Value(dtype='float32', id=None), 'nli_disgust': Value(dtype='float32', id=None), 'nli_sadness': Value(dtype='float32', id=None), 'nli_surprise': Value(dtype='float32', id=None), 'nli_trust': Value(dtype='float32', id=None), 'nli_anticipation': Value(dtype='float32', id=None), 'nli_grief': Value(dtype='float32', id=None)}\n",
      "Adding NLI scores to the dataset using (batched mode)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4e5a0764ab4417ba89b0b1012c7844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding NLI Scores:   0%|          | 0/2276118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI scores added to the dataset.\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None), 'nli_fear': Value(dtype='float32', id=None), 'nli_anger': Value(dtype='float32', id=None), 'nli_joy': Value(dtype='float32', id=None), 'nli_hostility': Value(dtype='float32', id=None), 'nli_hate': Value(dtype='float32', id=None), 'nli_love': Value(dtype='float32', id=None), 'nli_disgust': Value(dtype='float32', id=None), 'nli_sadness': Value(dtype='float32', id=None), 'nli_surprise': Value(dtype='float32', id=None), 'nli_trust': Value(dtype='float32', id=None), 'nli_anticipation': Value(dtype='float32', id=None), 'nli_grief': Value(dtype='float32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Define new features for NLI scores\n",
    "nli_features = {}\n",
    "for hypothesis in nli_hypotheses:\n",
    "    nli_features[f\"nli_{hypothesis}\"] = Value(\"float32\")\n",
    "\n",
    "# Copy existing features and add NLI features\n",
    "final_dataset_features_with_nli = ds_with_predictions.features.copy()\n",
    "final_dataset_features_with_nli.update(nli_features)\n",
    "\n",
    "print(\"\\nDefined final features for dataset after adding NLI scores:\")\n",
    "print(final_dataset_features_with_nli)\n",
    "\n",
    "print(f\"Adding NLI scores to the dataset using (batched mode)...\")\n",
    "ds_with_all_predictions = ds_with_predictions.map(\n",
    "    add_nli_scores_to_example_batched,\n",
    "    batched=True,             # Enable batching for NLI pipeline \n",
    "    batch_size=args.batch_size,\n",
    "    features=final_dataset_features_with_nli,\n",
    "    desc=\"Adding NLI Scores\"\n",
    ")\n",
    "\n",
    "print(\"NLI scores added to the dataset.\")\n",
    "print(ds_with_all_predictions.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b134719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_ground_truth_labels_batched(examples):\n",
    "    # 'examples' here will contain lists of tensors for 'labels' (e.g., [tensor([1,0,1]), tensor([0,0,1])])\n",
    "    batch_unpacked_data = {label_name: [] for label_name in labels}\n",
    "\n",
    "    for i in range(len(examples['labels'])): # Iterate through each example in the batch\n",
    "        # Convert tensor to list for the current example\n",
    "        gt_values = examples['labels'][i].tolist() if isinstance(examples['labels'][i], torch.Tensor) else examples['labels'][i]\n",
    "\n",
    "        if len(gt_values) != len(labels):\n",
    "            # This should ideally not happen if data is consistent, but good to check\n",
    "            raise ValueError(\n",
    "                f\"Mismatch for example in batch: 'labels' tensor has {len(gt_values)} values, \"\n",
    "                f\"but 'gt_label_names' has {len(labels)} names. \"\n",
    "                \"Ensure gt_label_names has exactly 40 elements in the correct order.\"\n",
    "            )\n",
    "\n",
    "        for j, label_name in enumerate(labels):\n",
    "            batch_unpacked_data[label_name].append(float(gt_values[j])) # Ensure it's a standard float\n",
    "\n",
    "    return batch_unpacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3d1c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unpacking ground truth labels into individual columns...\n",
      "\n",
      "Target features for dataset after unpacking GT labels (after removal):\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None), 'nli_fear': Value(dtype='float32', id=None), 'nli_anger': Value(dtype='float32', id=None), 'nli_joy': Value(dtype='float32', id=None), 'nli_hostility': Value(dtype='float32', id=None), 'nli_hate': Value(dtype='float32', id=None), 'nli_love': Value(dtype='float32', id=None), 'nli_disgust': Value(dtype='float32', id=None), 'nli_sadness': Value(dtype='float32', id=None), 'nli_surprise': Value(dtype='float32', id=None), 'nli_trust': Value(dtype='float32', id=None), 'nli_anticipation': Value(dtype='float32', id=None), 'nli_grief': Value(dtype='float32', id=None), 'post1geo10': Value(dtype='float32', id=None), 'post1geo20': Value(dtype='float32', id=None), 'post1geo30': Value(dtype='float32', id=None), 'post1geo50': Value(dtype='float32', id=None), 'post1geo70': Value(dtype='float32', id=None), 'post2geo10': Value(dtype='float32', id=None), 'post2geo20': Value(dtype='float32', id=None), 'post2geo30': Value(dtype='float32', id=None), 'post2geo50': Value(dtype='float32', id=None), 'post2geo70': Value(dtype='float32', id=None), 'post3geo10': Value(dtype='float32', id=None), 'post3geo20': Value(dtype='float32', id=None), 'post3geo30': Value(dtype='float32', id=None), 'post3geo50': Value(dtype='float32', id=None), 'post3geo70': Value(dtype='float32', id=None), 'post7geo10': Value(dtype='float32', id=None), 'post7geo20': Value(dtype='float32', id=None), 'post7geo30': Value(dtype='float32', id=None), 'post7geo50': Value(dtype='float32', id=None), 'post7geo70': Value(dtype='float32', id=None), 'pre1geo10': Value(dtype='float32', id=None), 'pre1geo20': Value(dtype='float32', id=None), 'pre1geo30': Value(dtype='float32', id=None), 'pre1geo50': Value(dtype='float32', id=None), 'pre1geo70': Value(dtype='float32', id=None), 'pre2geo10': Value(dtype='float32', id=None), 'pre2geo20': Value(dtype='float32', id=None), 'pre2geo30': Value(dtype='float32', id=None), 'pre2geo50': Value(dtype='float32', id=None), 'pre2geo70': Value(dtype='float32', id=None), 'pre3geo10': Value(dtype='float32', id=None), 'pre3geo20': Value(dtype='float32', id=None), 'pre3geo30': Value(dtype='float32', id=None), 'pre3geo50': Value(dtype='float32', id=None), 'pre3geo70': Value(dtype='float32', id=None), 'pre7geo10': Value(dtype='float32', id=None), 'pre7geo20': Value(dtype='float32', id=None), 'pre7geo30': Value(dtype='float32', id=None), 'pre7geo50': Value(dtype='float32', id=None), 'pre7geo70': Value(dtype='float32', id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f32dda04d84f66bfd7989ca0c93ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unpacking Ground Truth Labels:   0%|          | 0/2276118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth labels unpacked. Original 'labels' column removed.\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None), 'nli_fear': Value(dtype='float32', id=None), 'nli_anger': Value(dtype='float32', id=None), 'nli_joy': Value(dtype='float32', id=None), 'nli_hostility': Value(dtype='float32', id=None), 'nli_hate': Value(dtype='float32', id=None), 'nli_love': Value(dtype='float32', id=None), 'nli_disgust': Value(dtype='float32', id=None), 'nli_sadness': Value(dtype='float32', id=None), 'nli_surprise': Value(dtype='float32', id=None), 'nli_trust': Value(dtype='float32', id=None), 'nli_anticipation': Value(dtype='float32', id=None), 'nli_grief': Value(dtype='float32', id=None), 'post1geo10': Value(dtype='float32', id=None), 'post1geo20': Value(dtype='float32', id=None), 'post1geo30': Value(dtype='float32', id=None), 'post1geo50': Value(dtype='float32', id=None), 'post1geo70': Value(dtype='float32', id=None), 'post2geo10': Value(dtype='float32', id=None), 'post2geo20': Value(dtype='float32', id=None), 'post2geo30': Value(dtype='float32', id=None), 'post2geo50': Value(dtype='float32', id=None), 'post2geo70': Value(dtype='float32', id=None), 'post3geo10': Value(dtype='float32', id=None), 'post3geo20': Value(dtype='float32', id=None), 'post3geo30': Value(dtype='float32', id=None), 'post3geo50': Value(dtype='float32', id=None), 'post3geo70': Value(dtype='float32', id=None), 'post7geo10': Value(dtype='float32', id=None), 'post7geo20': Value(dtype='float32', id=None), 'post7geo30': Value(dtype='float32', id=None), 'post7geo50': Value(dtype='float32', id=None), 'post7geo70': Value(dtype='float32', id=None), 'pre1geo10': Value(dtype='float32', id=None), 'pre1geo20': Value(dtype='float32', id=None), 'pre1geo30': Value(dtype='float32', id=None), 'pre1geo50': Value(dtype='float32', id=None), 'pre1geo70': Value(dtype='float32', id=None), 'pre2geo10': Value(dtype='float32', id=None), 'pre2geo20': Value(dtype='float32', id=None), 'pre2geo30': Value(dtype='float32', id=None), 'pre2geo50': Value(dtype='float32', id=None), 'pre2geo70': Value(dtype='float32', id=None), 'pre3geo10': Value(dtype='float32', id=None), 'pre3geo20': Value(dtype='float32', id=None), 'pre3geo30': Value(dtype='float32', id=None), 'pre3geo50': Value(dtype='float32', id=None), 'pre3geo70': Value(dtype='float32', id=None), 'pre7geo10': Value(dtype='float32', id=None), 'pre7geo20': Value(dtype='float32', id=None), 'pre7geo30': Value(dtype='float32', id=None), 'pre7geo50': Value(dtype='float32', id=None), 'pre7geo70': Value(dtype='float32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnpacking ground truth labels into individual columns...\")\n",
    "\n",
    "# 1. Start with the features of ds_with_all_predictions\n",
    "target_features_for_unpacking = ds_with_all_predictions.features.copy()\n",
    "\n",
    "# 2. Explicitly remove columns that should *not* be in the final schema, including input_ids and attention_mask\n",
    "# This ensures your 'features' blueprint matches the final desired structure.\n",
    "columns_to_remove_from_schema = [\"labels\", \"input_ids\", \"attention_mask\"] # List all columns to be removed\n",
    "\n",
    "for col_name in columns_to_remove_from_schema:\n",
    "    if col_name in target_features_for_unpacking:\n",
    "        del target_features_for_unpacking[col_name]\n",
    "\n",
    "# 3. Add the new ground truth columns to this target schema\n",
    "# Assuming 'labels' is indeed your list of 40 ground truth label names as you stated.\n",
    "for name in labels: # Keeping 'labels' as per your confirmation\n",
    "    target_features_for_unpacking[name] = Value(\"float32\")\n",
    "\n",
    "print(\"\\nTarget features for dataset after unpacking GT labels (after removal):\")\n",
    "print(target_features_for_unpacking) # Print this to verify\n",
    "\n",
    "# Apply the unpacking map\n",
    "ds_final_for_csv = ds_with_all_predictions.map(\n",
    "    unpack_ground_truth_labels_batched, # Use the batched unpacking function\n",
    "    batched=True,                       # Apply in batched mode for performance\n",
    "    batch_size=args.batch_size,         # Use the same batch size\n",
    "    features=target_features_for_unpacking,\n",
    "    remove_columns=[\"labels\", \"input_ids\", \"attention_mask\"],    # Remove the original 'labels' Sequence column\n",
    "    desc=\"Unpacking Ground Truth Labels\"\n",
    ")\n",
    "\n",
    "print(\"Ground truth labels unpacked. Original 'labels' column removed.\")\n",
    "print(ds_final_for_csv.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b089b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All column names in the final dataset:\n",
      "['tweetid', 'geo_x', 'geo_y', 'lang', 'text', 'pred_post1geo10', 'pred_post1geo20', 'pred_post1geo30', 'pred_post1geo50', 'pred_post1geo70', 'pred_post2geo10', 'pred_post2geo20', 'pred_post2geo30', 'pred_post2geo50', 'pred_post2geo70', 'pred_post3geo10', 'pred_post3geo20', 'pred_post3geo30', 'pred_post3geo50', 'pred_post3geo70', 'pred_post7geo10', 'pred_post7geo20', 'pred_post7geo30', 'pred_post7geo50', 'pred_post7geo70', 'pred_pre1geo10', 'pred_pre1geo20', 'pred_pre1geo30', 'pred_pre1geo50', 'pred_pre1geo70', 'pred_pre2geo10', 'pred_pre2geo20', 'pred_pre2geo30', 'pred_pre2geo50', 'pred_pre2geo70', 'pred_pre3geo10', 'pred_pre3geo20', 'pred_pre3geo30', 'pred_pre3geo50', 'pred_pre3geo70', 'pred_pre7geo10', 'pred_pre7geo20', 'pred_pre7geo30', 'pred_pre7geo50', 'pred_pre7geo70', 'nli_fear', 'nli_anger', 'nli_joy', 'nli_hostility', 'nli_hate', 'nli_love', 'nli_disgust', 'nli_sadness', 'nli_surprise', 'nli_trust', 'nli_anticipation', 'nli_grief', 'post1geo10', 'post1geo20', 'post1geo30', 'post1geo50', 'post1geo70', 'post2geo10', 'post2geo20', 'post2geo30', 'post2geo50', 'post2geo70', 'post3geo10', 'post3geo20', 'post3geo30', 'post3geo50', 'post3geo70', 'post7geo10', 'post7geo20', 'post7geo30', 'post7geo50', 'post7geo70', 'pre1geo10', 'pre1geo20', 'pre1geo30', 'pre1geo50', 'pre1geo70', 'pre2geo10', 'pre2geo20', 'pre2geo30', 'pre2geo50', 'pre2geo70', 'pre3geo10', 'pre3geo20', 'pre3geo30', 'pre3geo50', 'pre3geo70', 'pre7geo10', 'pre7geo20', 'pre7geo30', 'pre7geo50', 'pre7geo70']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAll column names in the final dataset:\")\n",
    "all_final_column_names = list(ds_final_for_csv.features.keys())\n",
    "print(all_final_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f5a72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the final dataset (first row) before CSV export:\n",
      "tweetid: 388328898662268928\n",
      "text: talking abt my case ☺️\n",
      "pred_post1geo10: 0.012126137502491474\n",
      "nli_fear: 0.1177188903093338\n",
      "post1geo10: 0.0\n",
      "post1geo20: 0.0\n",
      "pre7geo70: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample of the final dataset (first row) before CSV export:\")\n",
    "if len(ds_final_for_csv) > 0:\n",
    "    first_sample_final = ds_final_for_csv[0]\n",
    "    # Print a few key original, prediction, NLI, and GT columns\n",
    "    print(f\"tweetid: {first_sample_final['tweetid']}\")\n",
    "    print(f\"text: {first_sample_final['text']}\")\n",
    "    print(f\"pred_post1geo10: {first_sample_final['pred_post1geo10']}\")\n",
    "    print(f\"nli_fear: {first_sample_final['nli_fear']}\")\n",
    "    print(f\"{labels[0]}: {first_sample_final[labels[0]]}\") # First GT label\n",
    "    print(f\"{labels[1]}: {first_sample_final[labels[1]]}\") # Second GT label\n",
    "    print(f\"{labels[-1]}: {first_sample_final[labels[-1]]}\") # Last GT label\n",
    "else:\n",
    "    print(\"Dataset is empty after unpacking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca54e7",
   "metadata": {},
   "source": [
    "## 4.5 Save the final dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6cdd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting final dataset to CSV: /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results_nli_multilabel_newCategories.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82f920d8f0341ac806111bd193c15fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2277 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset successfully exported to CSV: /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_e5_inference_results_nli_multilabel_newCategories.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nExporting final dataset to CSV: {args.fout_nli_csv}\")\n",
    "try:\n",
    "    ds_final_for_csv.to_csv(args.fout_nli_csv)\n",
    "    print(f\"Final dataset successfully exported to CSV: {args.fout_nli_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to export final dataset to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in pandas to check the first 3 observations\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path where you saved the CSV\n",
    "csv_file_path = args.fout_nli_csv # Or directly use the string path\n",
    "\n",
    "print(f\"\\nLoading CSV from: {csv_file_path}\")\n",
    "try:\n",
    "    # Load the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    print(f\"CSV loaded successfully. Total rows: {len(df)}\")\n",
    "\n",
    "    # Check the first 3 observations\n",
    "    print(\"\\nFirst 3 observations from the loaded CSV:\")\n",
    "    print(df.head(3))\n",
    "\n",
    "    # You can also check the columns and their data types in the DataFrame\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at {csv_file_path}. Please ensure the save operation was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load CSV from {csv_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdee0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
