{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27dd93d7-1abb-42fd-a86d-2a6012832a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650802df-9b6d-45ec-a1f4-92159f3061b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path_to_model_on_disk and model_ckpt\n",
    "\n",
    "# Options for path_to_model_on_disk:\n",
    "# \"/data3/mmendieta/models/xlmt_finetuned_twitter/worldly-blaze-2/epoch_14\"\n",
    "# \"/data3/mmendieta/models/smallLabse_finetuned_twitter/electric-glitter-32/epoch_15\"\n",
    "# \"/data3/mmendieta/models/labse_finetuned_twitter/dazzling-violet-5/epoch_19\"\n",
    "# \"/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels/\"\n",
    "\n",
    "# Options for model_ckpt\n",
    "# \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "# \"setu4993/LaBSE\"\n",
    "# \"setu4993/smaller-LaBSE\"\n",
    "# \"intfloat/multilingual-e5-large\"  # Hub\n",
    "# /data3/mmendieta/models/ml_e5_large  # local\n",
    "\n",
    "config = {\n",
    "    \"cuda_device\": 3,\n",
    "    \"path_to_model_on_disk\": \"/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/\", \n",
    "    \"model_ckpt\": \"setu4993/LaBSE\",\n",
    "    \"max_length\": 32\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9f6083-c89a-4e0f-b95b-35c5f6bcd180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "model_ckpt = args.model_ckpt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt,\n",
    "                                         model_max_length=args.max_length\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d3b78c-dae7-4e7c-a452-efb889733f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/tokenizer_config.json',\n",
       " '/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/special_tokens_map.json',\n",
       " '/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/vocab.txt',\n",
       " '/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/added_tokens.json',\n",
       " '/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(args.path_to_model_on_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13446c9c-12d4-43f5-b0f6-51f72ca2b148",
   "metadata": {},
   "source": [
    "### Hugging Face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a022c92-3702-47c3-807b-25855be7caa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6593581489346b484a665db47f3c2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, Repository, get_full_repo_name\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f78d7a0-7a4d-40ce-85db-72a87f501f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_full_repo_name, list_repo_refs, HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52de01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for your repository and checkpoint ---\n",
    "# Your local directory that will act as the Git repository root\n",
    "LOCAL_REPO_PATH = args.path_to_model_on_disk\n",
    "\n",
    "# Your Hugging Face Hub repository ID\n",
    "HUB_REPO_ID = \"m2im/ml-e5-large_finetuned_violence_twitter_all_labels\"\n",
    "\n",
    "# The full path to your epoch_18 checkpoint (source of model files)\n",
    "CHECKPOINT_PATH = os.path.join(LOCAL_REPO_PATH, \"fanciful-sunset-7\", \"epoch_18\")\n",
    "\n",
    "# Commit message for your push\n",
    "COMMIT_MESSAGE = \"Update main branch with recovered epoch_18 checkpoint and existing tokenizer\"\n",
    "\n",
    "# The target branch on the Hub\n",
    "BRANCH_TO_PUSH_TO = \"main\"\n",
    "\n",
    "# --- CRITICAL: CONFIRM THIS PATH IS CORRECT AFTER YOUR RECOVERY (PART 1) ---\n",
    "# This is the exact path where you confirmed 'config.json' and 'pytorch_model.bin' are located.\n",
    "RECOVERED_MODEL_SOURCE = \"/data4/mmendieta/recovered_models/fanciful-sunset-7/epoch_18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76dba85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Git repository object for existing local directory: '/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmendieta/transformers/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repository object initialized.\n",
      "Current local branch: main\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Initialize Repository object for the existing local directory ---\n",
    "# As per your request, we are NOT clearing or re-cloning LOCAL_REPO_PATH.\n",
    "# We are assuming it's already a valid Git repository connected to the Hub.\n",
    "print(f\"\\nInitializing Git repository object for existing local directory: '{LOCAL_REPO_PATH}'...\")\n",
    "hf_repo = Repository(\n",
    "    local_dir=LOCAL_REPO_PATH,\n",
    "    # No clone_from or revision, as we're working with an existing local repo\n",
    ")\n",
    "print(\"Git repository object initialized.\")\n",
    "print(f\"Current local branch: {hf_repo.current_branch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b3af81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying recovered MODEL files from '/data4/mmendieta/recovered_models/fanciful-sunset-7/epoch_18' to '/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels'...\n",
      "Copied MODEL file: config.json\n",
      "Copied MODEL file: pytorch_model.bin\n",
      "\n",
      "Using existing TOKENIZER files already present in '/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels' as requested.\n",
      "Please ensure all necessary tokenizer files (tokenizer.json, tokenizer_config.json,\n",
      "special_tokens_map.json, sentencepiece.bpe.model, and added_tokens.json if applicable)\n",
      "are already correctly present in this directory for the push.\n",
      "All necessary files (model + existing tokenizer) prepared in local repository for pushing.\n",
      "\n",
      "--- Contents of '/data4/mmendieta/models/ml-e5-large_finetuned_twitter_all_labels' AFTER all file preparation (BEFORE git add/commit/push) ---\n",
      "total 2209016\n",
      "-rw-rw-r-- 1 mmendieta mmendieta       2610 Jul 26 12:58 config.json\n",
      "-rw-rw-r-- 1 mmendieta mmendieta 2239862893 Jul 26 13:01 pytorch_model.bin\n",
      "-rw-rw-r-- 1 mmendieta mmendieta    5069051 Jul 26 12:29 sentencepiece.bpe.model\n",
      "-rw-rw-r-- 1 mmendieta mmendieta        239 Jul 26 12:29 special_tokens_map.json\n",
      "-rw-rw-r-- 1 mmendieta mmendieta        512 Jul 26 12:29 tokenizer_config.json\n",
      "-rw-rw-r-- 1 mmendieta mmendieta   17082660 Jul 26 12:29 tokenizer.json\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Ensure all necessary files (Model & Tokenizer) are in LOCAL_REPO_PATH ---\n",
    "\n",
    "# First, copy the recovered MODEL files from the checkpoint\n",
    "print(f\"\\nCopying recovered MODEL files from '{RECOVERED_MODEL_SOURCE}' to '{LOCAL_REPO_PATH}'...\")\n",
    "model_files_to_copy = [\n",
    "    \"config.json\",\n",
    "    \"pytorch_model.bin\", # Or 'model.safetensors', if that's what your model saved\n",
    "]\n",
    "for filename in model_files_to_copy:\n",
    "    src_file = os.path.join(RECOVERED_MODEL_SOURCE, filename)\n",
    "    dst_file = os.path.join(LOCAL_REPO_PATH, filename)\n",
    "    if os.path.exists(src_file):\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "        print(f\"Copied MODEL file: {filename}\")\n",
    "    else:\n",
    "        print(f\"WARNING: MODEL file '{filename}' not found in '{src_file}'. This might result in an incomplete model on the Hub.\")\n",
    "\n",
    "\n",
    "# Second, explicitly state that we are using the existing TOKENIZER files.\n",
    "print(f\"\\nUsing existing TOKENIZER files already present in '{LOCAL_REPO_PATH}' as requested.\")\n",
    "print(\"Please ensure all necessary tokenizer files (tokenizer.json, tokenizer_config.json,\")\n",
    "print(\"special_tokens_map.json, sentencepiece.bpe.model, and added_tokens.json if applicable)\")\n",
    "print(\"are already correctly present in this directory for the push.\")\n",
    "\n",
    "\n",
    "print(\"All necessary files (model + existing tokenizer) prepared in local repository for pushing.\")\n",
    "\n",
    "# --- DEBUGGING STEP: Print contents of LOCAL_REPO_PATH after all file preparation ---\n",
    "print(f\"\\n--- Contents of '{LOCAL_REPO_PATH}' AFTER all file preparation (BEFORE git add/commit/push) ---\")\n",
    "os.system(f\"ls -l {LOCAL_REPO_PATH}\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbb779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding files to Git staging area...\n",
      "Committing changes...\n",
      "Pushing changes to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/m2im/ml-e5-large_finetuned_violence_twitter_all_labels\n",
      "   802c163..9d9b915  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes pushed successfully to Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Add, Commit, and Push the changes to the Hub ---\n",
    "print(\"\\nAdding files to Git staging area...\")\n",
    "hf_repo.git_add(auto_lfs_track=True) # Automatically tracks large files with LFS\n",
    "print(\"Committing changes...\")\n",
    "hf_repo.git_commit(COMMIT_MESSAGE)\n",
    "print(\"Pushing changes to Hugging Face Hub...\")\n",
    "hf_repo.git_push(blocking=True) # blocking=True waits for the push to complete\n",
    "print(\"Changes pushed successfully to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f37bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying repository refs for m2im/ml-e5-large_finetuned_violence_twitter_all_labels...\n",
      "Branches on Hub:\n",
      "- main (Commit ID: 9d9b91526c272e0866a85882043a19ff3c9c082e)\n",
      "- fanciful-sunset-7 (Commit ID: 428e979290d71390ac7c7a3c3dfc81aa4136518d)\n",
      "\n",
      "Tags on Hub:\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Verify the push (Optional) ---\n",
    "# Use list_repo_refs to confirm the main branch's updated state\n",
    "print(f\"\\nVerifying repository refs for {HUB_REPO_ID}...\")\n",
    "try:\n",
    "    refs = list_repo_refs(HUB_REPO_ID)\n",
    "    print(\"Branches on Hub:\")\n",
    "    for branch in refs.branches:\n",
    "        # Corrected attribute: use 'target_commit' instead of 'target_commit_oid'\n",
    "        print(f\"- {branch.name} (Commit ID: {branch.target_commit})\")\n",
    "    print(\"\\nTags on Hub:\")\n",
    "    for tag in refs.tags:\n",
    "        # Corrected attribute: use 'target_commit' instead of 'target_commit_oid'\n",
    "        print(f\"- {tag.name} (Commit ID: {tag.target_commit})\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not list repository refs: {e}\")\n",
    "    print(\"Ensure the repository ID is correct and you have read access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238009a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7c059-da4b-4fb7-b440-50ef5d47d84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
