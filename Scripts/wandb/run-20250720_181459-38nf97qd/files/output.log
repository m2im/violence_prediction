Revision `decent-tree-31` does not exist. Created and checked out branch `decent-tree-31`.
D	genial-pond-21/epoch_0/config.json
D	genial-pond-21/epoch_0/pytorch_model.bin
D	genial-pond-21/epoch_1/config.json
D	genial-pond-21/epoch_1/pytorch_model.bin
D	genial-pond-21/epoch_2/config.json
D	genial-pond-21/epoch_2/pytorch_model.bin
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/mmendieta/.cache/huggingface/transformers/6e74f475a3ac17548fcd7f24a1116652919ba2225b290de8ef3e7d4ad36393fb.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer.json from cache at /home/mmendieta/.cache/huggingface/transformers/c949b67b285c79e2b38b50fb89ca8d00428266f364d9fe0c10721a0492dbbeba.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
[INFO] loading the tokenizer and the model ...
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "post1geo10",
    "1": "post1geo20",
    "2": "post1geo30",
    "3": "post1geo50",
    "4": "post1geo70",
    "5": "post2geo10",
    "6": "post2geo20",
    "7": "post2geo30",
    "8": "post2geo50",
    "9": "post2geo70",
    "10": "post3geo10",
    "11": "post3geo20",
    "12": "post3geo30",
    "13": "post3geo50",
    "14": "post3geo70",
    "15": "post7geo10",
    "16": "post7geo20",
    "17": "post7geo30",
    "18": "post7geo50",
    "19": "post7geo70",
    "20": "pre1geo10",
    "21": "pre1geo20",
    "22": "pre1geo30",
    "23": "pre1geo50",
    "24": "pre1geo70",
    "25": "pre2geo10",
    "26": "pre2geo20",
    "27": "pre2geo30",
    "28": "pre2geo50",
    "29": "pre2geo70",
    "30": "pre3geo10",
    "31": "pre3geo20",
    "32": "pre3geo30",
    "33": "pre3geo50",
    "34": "pre3geo70",
    "35": "pre7geo10",
    "36": "pre7geo20",
    "37": "pre7geo30",
    "38": "pre7geo50",
    "39": "pre7geo70"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "post1geo10": 0,
    "post1geo20": 1,
    "post1geo30": 2,
    "post1geo50": 3,
    "post1geo70": 4,
    "post2geo10": 5,
    "post2geo20": 6,
    "post2geo30": 7,
    "post2geo50": 8,
    "post2geo70": 9,
    "post3geo10": 10,
    "post3geo20": 11,
    "post3geo30": 12,
    "post3geo50": 13,
    "post3geo70": 14,
    "post7geo10": 15,
    "post7geo20": 16,
    "post7geo30": 17,
    "post7geo50": 18,
    "post7geo70": 19,
    "pre1geo10": 20,
    "pre1geo20": 21,
    "pre1geo30": 22,
    "pre1geo50": 23,
    "pre1geo70": 24,
    "pre2geo10": 25,
    "pre2geo20": 26,
    "pre2geo30": 27,
    "pre2geo50": 28,
    "pre2geo70": 29,
    "pre3geo10": 30,
    "pre3geo20": 31,
    "pre3geo30": 32,
    "pre3geo50": 33,
    "pre3geo70": 34,
    "pre7geo10": 35,
    "pre7geo20": 36,
    "pre7geo30": 37,
    "pre7geo50": 38,
    "pre7geo70": 39
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "multi_label_classification",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/mmendieta/.cache/huggingface/transformers/de8103490e7e03ee5861271a32167a4f170ce820c6ab3aba4df4d6b00fe65163.824322f81a7eced2993b9bf64487652aa08c7cd2a81b38e00b0464c51b97280d
Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
fine_tune_mllm_script_all_labels_accelerate.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label_tensor = torch.tensor(tokenized_ds["train"]["labels"])
327540
/home/mmendieta/transformers/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO] training starts...
[INFO] Starting epoch 0
  0%|                                                   | 0/20 [00:00<?, ?it/s]
[DEBUG] - Processing step 1/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1001/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1024/len(train_dl) on device cuda:0
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 256
[INFO] Processed eval batch 1/256
[INFO] Processed eval batch 101/256
[INFO] Processed eval batch 201/256

  0%|                                                   | 0/20 [00:00<?, ?it/s]Configuration saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_0/config.json
Model weights saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_0/pytorch_model.bin
Several commits (4) will be pushed upstream.
WARNING:huggingface_hub.repository:Several commits (4) will be pushed upstream.
The progress bars may be unreliable.
WARNING:huggingface_hub.repository:The progress bars may be unreliable.
Upload file decent-tree-31/epoch_0/pytorch_model.bin:   0%| | 1.00/1.04G [00:00
Upload file genial-pond-21/epoch_1/pytorch_model.bin:   0%| | 1.00/1.04G [00:00
Upload file genial-pond-21/epoch_0/pytorch_model.bin:   0%| | 1.00/1.04G [00:00































Upload file genial-pond-21/epoch_2/pytorch_model.bin:  99%|▉| 1.03G/1.04G [01:0
[INFO] Per-label metrics written to: /data3/mmendieta/Violence_data/training_output/xlmt/per_label_metrics.json
[epoch 0] train/loss: 0.9013 | eval/loss: 0.6563 | roc_auc_micro: 0.6488 | roc_auc_weighted: 0.6380 | precision_micro: 0.3970 | precision_weighted: 0.4485 | recall_micro: 0.5717 | recall_weighted: 0.5717 | f1_micro: 0.4686 | f1_weighted: 0.4859 | elapsed_time: 1783.93s
 * [new branch]      decent-tree-31 -> decent-tree-31  99%|▉| 1.03G/1.04G [01:0
Upload file decent-tree-31/epoch_0/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
Upload file genial-pond-21/epoch_1/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
Upload file genial-pond-21/epoch_0/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
Upload file genial-pond-21/epoch_2/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
  5%|█▉                                    | 1/20 [32:19<10:14:18, 1939.90s/it]
Upload file genial-pond-21/epoch_0/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
Upload file genial-pond-21/epoch_2/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
[DEBUG] - Processing step 1/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1001/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1024/len(train_dl) on device cuda:0
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 256
[INFO] Processed eval batch 1/256
[INFO] Processed eval batch 101/256
[INFO] Processed eval batch 201/256
WARNING:huggingface_hub.repository:Several commits (5) will be pushed upstream.
The progress bars may be unreliable.
WARNING:huggingface_hub.repository:The progress bars may be unreliable.
































Upload file decent-tree-31/epoch_1/pytorch_model.bin: 1.04GB [01:04, 18.1MB/s]
[INFO] Per-label metrics written to: /data3/mmendieta/Violence_data/training_output/xlmt/per_label_metrics.json
[epoch 1] train/loss: 0.9151 | eval/loss: 0.6420 | roc_auc_micro: 0.6640 | roc_auc_weighted: 0.6522 | precision_micro: 0.4075 | precision_weighted: 0.4579 | recall_micro: 0.5734 | recall_weighted: 0.5734 | f1_micro: 0.4764 | f1_weighted: 0.4933 | elapsed_time: 3709.35s
   5f0fac7..740a1aa  decent-tree-31 -> decent-tree-31 1.04GB [01:04, 18.1MB/s]
WARNING:huggingface_hub.repository:To https://huggingface.co/m2im/XLM-T_finetuned_violence_twitter_all_labels
   5f0fac7..740a1aa  decent-tree-31 -> decent-tree-31
Upload file decent-tree-31/epoch_1/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:2
 10%|███▋                                 | 2/20 [1:03:30<9:29:46, 1899.22s/it]
[DEBUG] - Processing step 1/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1001/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1024/len(train_dl) on device cuda:0
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 256
[INFO] Processed eval batch 1/256
[INFO] Processed eval batch 101/256
[INFO] Processed eval batch 201/256

 10%|███▋                                 | 2/20 [1:03:30<9:29:46, 1899.22s/it]Configuration saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_2/config.json
Model weights saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_2/pytorch_model.bin
Several commits (6) will be pushed upstream.
WARNING:huggingface_hub.repository:Several commits (6) will be pushed upstream.
The progress bars may be unreliable.
WARNING:huggingface_hub.repository:The progress bars may be unreliable.































Upload file decent-tree-31/epoch_2/pytorch_model.bin: 1.04GB [01:03, 16.1MB/s]
[INFO] Per-label metrics written to: /data3/mmendieta/Violence_data/training_output/xlmt/per_label_metrics.json
[epoch 2] train/loss: 0.8713 | eval/loss: 0.6368 | roc_auc_micro: 0.6725 | roc_auc_weighted: 0.6594 | precision_micro: 0.4132 | precision_weighted: 0.4625 | recall_micro: 0.5837 | recall_weighted: 0.5837 | f1_micro: 0.4839 | f1_weighted: 0.4995 | elapsed_time: 5567.31s
   740a1aa..4148c60  decent-tree-31 -> decent-tree-31 1.04GB [01:03, 16.1MB/s]
WARNING:huggingface_hub.repository:To https://huggingface.co/m2im/XLM-T_finetuned_violence_twitter_all_labels
   740a1aa..4148c60  decent-tree-31 -> decent-tree-31
Upload file decent-tree-31/epoch_2/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:1
 15%|█████▌                               | 3/20 [1:34:38<8:54:05, 1885.03s/it]
[DEBUG] - Processing step 1/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1001/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1024/len(train_dl) on device cuda:0
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 256
[INFO] Processed eval batch 1/256
[INFO] Processed eval batch 101/256
[INFO] Processed eval batch 201/256

 15%|█████▌                               | 3/20 [1:34:38<8:54:05, 1885.03s/it]Configuration saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_3/config.json
Model weights saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_3/pytorch_model.bin
Several commits (7) will be pushed upstream.
WARNING:huggingface_hub.repository:Several commits (7) will be pushed upstream.
The progress bars may be unreliable.
WARNING:huggingface_hub.repository:The progress bars may be unreliable.

































   4148c60..8170efc  decent-tree-31 -> decent-tree-31 1.04GB [01:07, 19.5MB/s]
WARNING:huggingface_hub.repository:To https://huggingface.co/m2im/XLM-T_finetuned_violence_twitter_all_labels
   4148c60..8170efc  decent-tree-31 -> decent-tree-31
Upload file decent-tree-31/epoch_3/pytorch_model.bin: 100%|█| 1.04G/1.04G [01:0
 20%|███████▍                             | 4/20 [2:05:39<8:20:05, 1875.32s/it]
[INFO] Per-label metrics written to: /data3/mmendieta/Violence_data/training_output/xlmt/per_label_metrics.json
[epoch 3] train/loss: 0.8865 | eval/loss: 0.6245 | roc_auc_micro: 0.6796 | roc_auc_weighted: 0.6648 | precision_micro: 0.4233 | precision_weighted: 0.4739 | recall_micro: 0.5491 | recall_weighted: 0.5491 | f1_micro: 0.4781 | f1_weighted: 0.4927 | elapsed_time: 7436.40s
[INFO] Starting epoch 4
[DEBUG] - Processing step 1/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1001/len(train_dl) on device cuda:0
[DEBUG] - Processing step 1024/len(train_dl) on device cuda:0
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 256
[INFO] Processed eval batch 1/256
[INFO] Processed eval batch 101/256
[INFO] Processed eval batch 201/256

 20%|███████▍                             | 4/20 [2:05:39<8:20:05, 1875.32s/it]Configuration saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_4/config.json
Model weights saved in /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/decent-tree-31/epoch_4/pytorch_model.bin
Traceback (most recent call last):
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/huggingface_hub/repository.py", line 1128, in git_add
    result = run_subprocess("git add -v".split() + [pattern], self.local_dir)
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/huggingface_hub/utils/_subprocess.py", line 52, in run_subprocess
    return subprocess.run(
  File "/usr/local/miniconda3/lib/python3.8/subprocess.py", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['git', 'add', '-v', '.']' returned non-zero exit status 128.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "fine_tune_mllm_script_all_labels_accelerate.py", line 314, in <module>
    hf_repo.push_to_hub(commit_message=f"epoch_{epoch}")
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/huggingface_hub/repository.py", line 1436, in push_to_hub
    self.git_add(auto_lfs_track=True)
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/huggingface_hub/repository.py", line 1131, in git_add
    raise EnvironmentError(exc.stderr)
OSError: Error cleaning Git LFS object: write /data3/mmendieta/models/xlmt_finetuned_twitter_all_labels/.git/lfs/tmp/807840880: no space left on device
error: packet write failed
error: external filter 'git-lfs filter-process' failed
fatal: decent-tree-31/epoch_4/pytorch_model.bin: clean filter 'lfs' failed