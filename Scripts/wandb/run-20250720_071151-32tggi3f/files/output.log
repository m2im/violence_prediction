Revision `sunny-waterfall-20` does not exist. Created and checked out branch `sunny-waterfall-20`.
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/mmendieta/.cache/huggingface/transformers/6e74f475a3ac17548fcd7f24a1116652919ba2225b290de8ef3e7d4ad36393fb.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer.json from cache at /home/mmendieta/.cache/huggingface/transformers/c949b67b285c79e2b38b50fb89ca8d00428266f364d9fe0c10721a0492dbbeba.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
[INFO] loading the tokenizer and the model ...
loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /home/mmendieta/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2
Model config XLMRobertaConfig {
  "_name_or_path": "cardiffnlp/twitter-xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "post1geo10",
    "1": "post1geo20",
    "2": "post1geo30",
    "3": "post1geo50",
    "4": "post1geo70",
    "5": "post2geo10",
    "6": "post2geo20",
    "7": "post2geo30",
    "8": "post2geo50",
    "9": "post2geo70",
    "10": "post3geo10",
    "11": "post3geo20",
    "12": "post3geo30",
    "13": "post3geo50",
    "14": "post3geo70",
    "15": "post7geo10",
    "16": "post7geo20",
    "17": "post7geo30",
    "18": "post7geo50",
    "19": "post7geo70",
    "20": "pre1geo10",
    "21": "pre1geo20",
    "22": "pre1geo30",
    "23": "pre1geo50",
    "24": "pre1geo70",
    "25": "pre2geo10",
    "26": "pre2geo20",
    "27": "pre2geo30",
    "28": "pre2geo50",
    "29": "pre2geo70",
    "30": "pre3geo10",
    "31": "pre3geo20",
    "32": "pre3geo30",
    "33": "pre3geo50",
    "34": "pre3geo70",
    "35": "pre7geo10",
    "36": "pre7geo20",
    "37": "pre7geo30",
    "38": "pre7geo50",
    "39": "pre7geo70"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "post1geo10": 0,
    "post1geo20": 1,
    "post1geo30": 2,
    "post1geo50": 3,
    "post1geo70": 4,
    "post2geo10": 5,
    "post2geo20": 6,
    "post2geo30": 7,
    "post2geo50": 8,
    "post2geo70": 9,
    "post3geo10": 10,
    "post3geo20": 11,
    "post3geo30": 12,
    "post3geo50": 13,
    "post3geo70": 14,
    "post7geo10": 15,
    "post7geo20": 16,
    "post7geo30": 17,
    "post7geo50": 18,
    "post7geo70": 19,
    "pre1geo10": 20,
    "pre1geo20": 21,
    "pre1geo30": 22,
    "pre1geo50": 23,
    "pre1geo70": 24,
    "pre2geo10": 25,
    "pre2geo20": 26,
    "pre2geo30": 27,
    "pre2geo50": 28,
    "pre2geo70": 29,
    "pre3geo10": 30,
    "pre3geo20": 31,
    "pre3geo30": 32,
    "pre3geo50": 33,
    "pre3geo70": 34,
    "pre7geo10": 35,
    "pre7geo20": 36,
    "pre7geo30": 37,
    "pre7geo50": 38,
    "pre7geo70": 39
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "multi_label_classification",
  "transformers_version": "4.19.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}
loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/mmendieta/.cache/huggingface/transformers/de8103490e7e03ee5861271a32167a4f170ce820c6ab3aba4df4d6b00fe65163.824322f81a7eced2993b9bf64487652aa08c7cd2a81b38e00b0464c51b97280d
Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
30
fine_tune_mllm_script_all_labels_accelerate.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label_tensor = torch.tensor(tokenized_ds["train"]["labels"])
/home/mmendieta/transformers/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO] training starts...
  0%|                                                     | 0/3 [00:00<?, ?it/s]
[INFO] evaluating and saving model checkpoint...
[DEBUG] Starting evaluation... Number of batches: 9
[DEBUG] Processing batch 1
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.45497071743011475, max=0.5360147356987, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [123. 136. 154. 193. 244. 180. 200. 224. 274. 334. 219. 242. 273. 339.
 408. 357. 404. 468. 581. 662. 139. 153. 175. 217. 266. 181. 202. 230.
 284. 352. 223. 248. 280. 351. 430. 396. 447. 491. 606. 710.]
[DEBUG] Successfully added 1/9 batches to metrics.
[DEBUG] Processing batch 2
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.459193617105484, max=0.5387474894523621, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [118. 133. 158. 188. 240. 178. 203. 229. 266. 335. 211. 241. 276. 329.
 405. 350. 407. 465. 566. 665. 116. 133. 162. 199. 252. 153. 178. 219.
 283. 348. 194. 229. 276. 356. 430. 343. 408. 470. 592. 709.]
[DEBUG] Successfully added 2/9 batches to metrics.
[DEBUG] Processing batch 3
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.45322614908218384, max=0.5401466488838196, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [105. 117. 141. 190. 243. 147. 164. 203. 269. 330. 188. 210. 256. 341.
 410. 318. 364. 434. 568. 668. 107. 123. 147. 191. 244. 152. 176. 210.
 269. 335. 185. 213. 252. 331. 412. 343. 399. 454. 586. 698.]
[DEBUG] Successfully added 3/9 batches to metrics.
[DEBUG] Processing batch 4
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.4545482397079468, max=0.5403804183006287, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [116. 132. 158. 196. 238. 155. 182. 221. 276. 337. 197. 229. 280. 354.
 423. 334. 390. 453. 571. 657. 122. 133. 156. 190. 238. 162. 182. 217.
 266. 329. 199. 220. 264. 327. 400. 376. 415. 478. 597. 700.]
[DEBUG] Successfully added 4/9 batches to metrics.
[DEBUG] Processing batch 5
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.45606881380081177, max=0.5415385961532593, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [ 96. 110. 135. 179. 224. 131. 154. 189. 248. 302. 176. 201. 248. 320.
 391. 320. 369. 442. 565. 661. 118. 136. 161. 211. 264. 166. 185. 217.
 288. 356. 208. 234. 270. 351. 434. 367. 416. 469. 598. 707.]
[DEBUG] Successfully added 5/9 batches to metrics.
[DEBUG] Processing batch 6
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.4559514820575714, max=0.5366405844688416, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [108. 129. 159. 197. 254. 148. 173. 218. 271. 337. 196. 237. 292. 369.
 439. 338. 395. 468. 602. 693. 104. 124. 145. 189. 248. 135. 158. 183.
 247. 321. 169. 194. 230. 314. 393. 326. 374. 426. 554. 665.]
[DEBUG] Successfully added 6/9 batches to metrics.
[DEBUG] Processing batch 7
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.44921404123306274, max=0.53663170337677, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [122. 133. 164. 205. 262. 172. 189. 229. 286. 350. 214. 243. 289. 363.
 439. 351. 401. 456. 572. 673. 125. 143. 169. 210. 264. 164. 188. 214.
 280. 346. 203. 236. 267. 345. 418. 382. 432. 479. 604. 710.]
[DEBUG] Successfully added 7/9 batches to metrics.
[DEBUG] Processing batch 8
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.45648154616355896, max=0.5400841236114502, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [123. 137. 156. 192. 244. 166. 188. 218. 270. 333. 204. 230. 266. 328.
 403. 345. 389. 449. 557. 656. 112. 128. 152. 185. 245. 161. 189. 214.
 264. 343. 204. 236. 275. 339. 429. 364. 417. 473. 582. 699.]
[DEBUG] Successfully added 8/9 batches to metrics.
[DEBUG] Processing batch 9
[DEBUG] gathered_probs.shape: (1024, 40), gathered_labels.shape: (1024, 40)
[DEBUG] gathered_probs.min=0.45810070633888245, max=0.5353111624717712, dtype=float32
[DEBUG] gathered_labels.sum(axis=0): [126. 137. 173. 218. 260. 166. 183. 227. 288. 342. 211. 237. 284. 355.
 415. 359. 409. 467. 578. 673. 121. 136. 166. 207. 260. 184. 202. 235.
 293. 358. 217. 236. 269. 349. 428. 385. 422. 476. 596. 699.]
  0%|                                                     | 0/3 [00:00<?, ?it/s]Traceback (most recent call last):
  File "fine_tune_mllm_script_all_labels_accelerate.py", line 355, in <module>
    metrics_dict, eval_loss = evaluate_fn(args)
  File "fine_tune_mllm_script_all_labels_accelerate.py", line 278, in evaluate_fn
    "precision": precision_metric.compute(average=None)["precision"] if precision_batches > 0 else [0.0] * len(labels),
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/evaluate/module.py", line 433, in compute
    self._finalize()
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/evaluate/module.py", line 385, in _finalize
    file_paths, filelocks = self._get_all_cache_files()
  File "/home/mmendieta/transformers/lib/python3.8/site-packages/evaluate/module.py", line 302, in _get_all_cache_files
    raise ValueError(
ValueError: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.
[ERROR] Failed to compute other metrics: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.