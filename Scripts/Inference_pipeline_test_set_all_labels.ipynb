{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555a6130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 23:45:13.339030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk, Value, Dataset, Features, Sequence, ClassLabel\n",
    "from argparse import Namespace\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3125ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['post1geo10', 'post1geo20', 'post1geo30', 'post1geo50', 'post1geo70', 'post2geo10', 'post2geo20', \n",
    "          'post2geo30', 'post2geo50', 'post2geo70', 'post3geo10', 'post3geo20', 'post3geo30', 'post3geo50', \n",
    "          'post3geo70', 'post7geo10', 'post7geo20', 'post7geo30', 'post7geo50', 'post7geo70', 'pre1geo10', \n",
    "          'pre1geo20', 'pre1geo30', 'pre1geo50', 'pre1geo70', 'pre2geo10', 'pre2geo20', 'pre2geo30', \n",
    "          'pre2geo50', 'pre2geo70', 'pre3geo10', 'pre3geo20', 'pre3geo30', 'pre3geo50', 'pre3geo70', \n",
    "          'pre7geo10', 'pre7geo20', 'pre7geo30', 'pre7geo50', 'pre7geo70']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2a9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically extract label names from dataset and create mappings\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb542b",
   "metadata": {},
   "source": [
    "# 1. Prepare the dataset for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cdb25",
   "metadata": {},
   "source": [
    "### Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b747c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very expensive operation.  It takes approximately 20 min to run\n",
    "# At runtime, this command stores the entire dataset in /tmp.  This takes about 0.8 T of disk space.\n",
    "# After using the dataset, delete the temp directory from /tmp, but this requires sudo priviledges\n",
    "ds_splits = load_from_disk(\"/data3/mmendieta/Violence_data/geo_corpus.0.0.1_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a046b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a638f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at one sample\n",
    "ds_splits[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1af3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca422d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unncesary columns\n",
    "columns_to_remove = ['retweetid', 'date', 'timestamp', 'username', 'key']\n",
    "\n",
    "# Use the .remove_columns() method\n",
    "ds_test = ds_test.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92244d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = ds_test.features.copy()\n",
    "\n",
    "new_features['tweetid'] = Value(dtype='string')\n",
    "\n",
    "# Iterate directly through your list of labels to cast them\n",
    "for col_name in labels:\n",
    "    # It's good practice to check if the column exists, though with a pre-defined list,\n",
    "    # it's usually assumed all columns in the list exist.\n",
    "    if col_name in new_features:\n",
    "        new_features[col_name] = Value(dtype='float32')\n",
    "    else:\n",
    "        # This part will only execute if a column in your 'labels' list is missing\n",
    "        print(f\"Warning: Column '{col_name}' from the 'labels' list not found in dataset features.\")\n",
    "\n",
    "# Cast the dataset with the new features\n",
    "# Since 'ds_test' is a single Dataset object, apply the .cast() method directly.\n",
    "ds_test = ds_test.cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71758b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approximately 14 min to run\n",
    "# It is important that the labels are float in order to calculate Binary Cross Entropy loss\n",
    "# create 'labels' columm\n",
    "\n",
    "# Define columns to ignore\n",
    "ignore_columns = [\"tweetid\", \"geo_x\", \"geo_y\", \"lang\", \"text\"]\n",
    "\n",
    "# Filter to only work on the test set\n",
    "cols = [col for col in ds_test.column_names if col not in ignore_columns]\n",
    "\n",
    "# Map function to create labels\n",
    "ds_test = ds_test.map(lambda x: {\"labels\": [x[c] for c in cols]}, remove_columns=cols)\n",
    "\n",
    "ds_test                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2edfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e336a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.save_to_disk(\"/data4/mmendieta/data/geo_corpus.0.0.1_test_dataset_for_inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71de09",
   "metadata": {},
   "source": [
    "# 2. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17047020",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/data4/mmendieta/data/geo_corpus.0.0.1_test_dataset_for_inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model_ckpt\": \"/data3/mmendieta/models/ml_e5_large/\"\n",
    "# \"model_ckpt\": \"setu4993/LaBSE\"\n",
    "# \"model_ckpt\": \"setu4993/smaller-LaBSE\"\n",
    "# \"model_ckpt\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "# \"fout\": \"/data3/mmendieta/Violence_data/geo_corpus.0.0.1_tok_test_ds_e5_inference\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_ckpt\": \"setu4993/LaBSE\",\n",
    "    \"batch_size\": 1024,\n",
    "    \"num_labels\" : 40,\n",
    "    \"max_length\": 32,\n",
    "    \"seed\": 42,\n",
    "    \"fout\": \"/data3/mmendieta/Violence_data/geo_corpus.0.0.1_tok_test_ds_labse_inference\"\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "model_ckpt = args.model_ckpt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, \n",
    "                                              model_max_length=args.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c57a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes 2 min to run\n",
    "%time tokenized_ds = ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d51008",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.save_to_disk(args.fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944dfae",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1891a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cuda_device\": 2,\n",
    "    \"path_to_model_on_disk\": \"/data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/\", \n",
    "    \"model_ckpt\": \"\",\n",
    "    \"max_length\": 32,\n",
    "    \"dataset_name\": \"/data3/mmendieta/Violence_data/geo_corpus.0.0.1_tok_test_ds_labse_inference\",\n",
    "    \"batch_size\": 1024,\n",
    "    \"fout_inference\": \"/data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_labse_inference_results\"\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb2a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /data3/mmendieta/Violence_data/geo_corpus.0.0.1_tok_test_ds_labse_inference\n",
      "Features of the loaded dataset: {'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "\n",
      "DEBUG: After casting, ds_tok[0]['labels'] type: <class 'torch.Tensor'>\n",
      "DEBUG: After casting, ds_tok[0]['labels'] value: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1.])\n",
      "DEBUG: After casting, ds_tok[0]['labels'] .shape: torch.Size([40])\n",
      "DEBUG: After casting, ds_tok[0]['labels'] .ndim: 1\n"
     ]
    }
   ],
   "source": [
    "# Recall that the test dataset has 2.329.158 observations. The inference is done in batches to exploit GPU resources\n",
    "print(f\"Loading dataset from: {args.dataset_name}\")\n",
    "try:\n",
    "    # LOAD YOUR DATASET FROM DISK\n",
    "    ds_tok = load_from_disk(args.dataset_name)\n",
    "\n",
    "    print(\"Features of the loaded dataset:\", ds_tok.features)\n",
    "    \n",
    "    # --- NEW DEBUG: Inspect example 0 ---\n",
    "    if len(ds_tok) > 0:\n",
    "        first_example_labels = ds_tok[0]['labels']\n",
    "        print(f\"\\nDEBUG: After casting, ds_tok[0]['labels'] type: {type(first_example_labels)}\")\n",
    "        print(f\"DEBUG: After casting, ds_tok[0]['labels'] value: {first_example_labels}\")\n",
    "        if isinstance(first_example_labels, np.ndarray):\n",
    "            print(f\"DEBUG: After casting, ds_tok[0]['labels'] shape: {first_example_labels.shape}\")\n",
    "        if isinstance(first_example_labels, torch.Tensor):\n",
    "            print(f\"DEBUG: After casting, ds_tok[0]['labels'] .shape: {first_example_labels.shape}\")\n",
    "            print(f\"DEBUG: After casting, ds_tok[0]['labels'] .ndim: {first_example_labels.ndim}\")\n",
    "    # --- END NEW DEBUG ---\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load or cast dataset from {args.dataset_name}: {e}\")\n",
    "    print(\"Please ensure your actual dataset exists at the specified path and its content is compatible with the defined 'initial_features'.\")\n",
    "    raise # Re-raise the exception to stop execution if actual loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90347956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current features of your loaded dataset:\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "\n",
      "Feature for 'labels' column: Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)\n",
      "The 'labels' column is a Sequence (list-like).\n",
      "  Inner feature type: Value(dtype='float64', id=None)\n",
      "  It contains Value objects (e.g., integers, floats). Dtype: float64\n",
      "\n",
      "First 5 entries of the 'labels' column (raw data):\n",
      "  Example 0: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1.])\n",
      "  Example 1: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1.])\n",
      "  Example 2: tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "  Example 3: tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1.])\n",
      "  Example 4: tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(\"Current features of your loaded dataset:\")\n",
    "print(ds_tok.features)\n",
    "\n",
    "# Now, specifically look at the 'labels' entry\n",
    "if 'labels' in ds_tok.features:\n",
    "    labels_feature = ds_tok.features['labels']\n",
    "    print(f\"\\nFeature for 'labels' column: {labels_feature}\")\n",
    "\n",
    "    if isinstance(labels_feature, Sequence):\n",
    "        print(\"The 'labels' column is a Sequence (list-like).\")\n",
    "        inner_feature = labels_feature.feature\n",
    "        print(f\"  Inner feature type: {inner_feature}\")\n",
    "        if isinstance(inner_feature, ClassLabel):\n",
    "            print(f\"  It contains ClassLabel objects (strings that map to predefined names). Names: {inner_feature.names}\")\n",
    "        elif isinstance(inner_feature, Value):\n",
    "            print(f\"  It contains Value objects (e.g., integers, floats). Dtype: {inner_feature.dtype}\")\n",
    "        else:\n",
    "            print(\"  Unknown inner feature type for Sequence.\")\n",
    "    elif isinstance(labels_feature, ClassLabel):\n",
    "        print(f\"The 'labels' column contains ClassLabel objects (single strings mapping to predefined names). Names: {labels_feature.names}\")\n",
    "    elif isinstance(labels_feature, Value):\n",
    "        print(f\"The 'labels' column contains single Value objects (e.g., integers, floats). Dtype: {labels_feature.dtype}\")\n",
    "    else:\n",
    "        print(\"The 'labels' column has an unexpected feature type.\")\n",
    "else:\n",
    "    print(\"The 'labels' column does not exist in your dataset's features.\")\n",
    "\n",
    "# Also inspect the first few raw data entries to be absolutely sure\n",
    "print(\"\\nFirst 5 entries of the 'labels' column (raw data):\")\n",
    "for i in range(min(5, len(ds_tok))):\n",
    "    print(f\"  Example {i}: {ds_tok[i]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the full dataset, make sure to change ds_tok_sample by ds_tok\n",
    "# First, shuffle the dataset\n",
    "ds_tok_shuffled = ds_tok.shuffle(seed=42) # Using a seed for reproducibility\n",
    "\n",
    "# Then, select the first 10,000 observations from the shuffled dataset\n",
    "ds_tok_sample = ds_tok_shuffled.select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c06303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting 'labels' column to plain Python lists of floats...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2afeee862f4589ad3e3bd0574ecfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting labels to list:   0%|          | 0/2329158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUGGING EXAMPLE 999 in convert_labels_to_list ---\n",
      "Labels BEFORE conversion: Type=<class 'torch.Tensor'>, Value=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1.])\n",
      "Labels BEFORE conversion: Shape=torch.Size([40]), NDIM=1\n",
      "Labels AFTER conversion: Type=<class 'list'>, Value=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "--- END DEBUGGING EXAMPLE 999 ---\n",
      "'labels' column conversion complete.\n",
      "Sample dataset features AFTER label conversion: {'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# --- CRITICAL NEW STEP: Ensure labels are plain Python lists after casting ---\n",
    "# This is the key fix given the PyTorch Tensor issue.\n",
    "# We convert the labels column to a pure Python list of floats.\n",
    "# This code takes approximately 30 min with the full test dataset\n",
    "\n",
    "def convert_labels_to_list(example, idx): # Add idx for debugging\n",
    "    original_labels = example['labels']\n",
    "\n",
    "    if idx == 999: # Focus on the problematic example\n",
    "        print(f\"\\n--- DEBUGGING EXAMPLE {idx} in convert_labels_to_list ---\")\n",
    "        print(f\"Labels BEFORE conversion: Type={type(original_labels)}, Value={original_labels}\")\n",
    "        if isinstance(original_labels, (np.ndarray, torch.Tensor)):\n",
    "            print(f\"Labels BEFORE conversion: Shape={original_labels.shape}, NDIM={original_labels.ndim}\")\n",
    "\n",
    "\n",
    "    if isinstance(original_labels, torch.Tensor):\n",
    "        # Always convert to list of floats, specifically handling 0-D tensors if they appear\n",
    "        if original_labels.ndim == 0: # Handle scalar tensor\n",
    "            converted_labels = [original_labels.item()]\n",
    "        else: # Handle 1-D or higher-D tensors\n",
    "            converted_labels = original_labels.detach().cpu().numpy().tolist()\n",
    "    elif isinstance(original_labels, np.ndarray):\n",
    "        if original_labels.ndim == 0: # Handle scalar numpy array\n",
    "            converted_labels = [original_labels.item()]\n",
    "        else: # Handle 1-D or higher-D numpy arrays\n",
    "            converted_labels = original_labels.tolist()\n",
    "    elif isinstance(original_labels, (float, int)): # Directly handle Python scalars\n",
    "        converted_labels = [float(original_labels)]\n",
    "    else: # Assume it's already a list or other iterable, or raise an error if unexpected\n",
    "        converted_labels = original_labels # Keep as is, or convert if needed\n",
    "        if not isinstance(converted_labels, list):\n",
    "             # This means it's neither tensor, numpy array, float, int, nor list.\n",
    "             # This case should ideally not happen if features are correctly casted.\n",
    "             print(f\"WARNING: Example {idx}, 'labels' is an unexpected type: {type(original_labels)}. Attempting to convert to list.\")\n",
    "             try:\n",
    "                 converted_labels = list(original_labels)\n",
    "             except TypeError:\n",
    "                 raise TypeError(f\"Example {idx}, 'labels' could not be converted to a list: {original_labels}\")\n",
    "\n",
    "    # Final check to ensure it's a list\n",
    "    if not isinstance(converted_labels, list):\n",
    "        raise TypeError(f\"CRITICAL ERROR: Example {idx}, 'labels' is not a list AFTER conversion: \"\n",
    "                        f\"Type={type(converted_labels)}, Value={converted_labels}\")\n",
    "    # Ensure all elements in the list are floats\n",
    "    if not all(isinstance(x, (float, int)) for x in converted_labels):\n",
    "        print(f\"WARNING: Example {idx}, 'labels' list contains non-numeric elements: {converted_labels}\")\n",
    "        # Attempt to cast all elements to float if possible\n",
    "        converted_labels = [float(x) for x in converted_labels]\n",
    "\n",
    "    example['labels'] = converted_labels\n",
    "\n",
    "    if idx == 999:\n",
    "        print(f\"Labels AFTER conversion: Type={type(example['labels'])}, Value={example['labels']}\")\n",
    "        print(f\"--- END DEBUGGING EXAMPLE {idx} ---\")\n",
    "\n",
    "    return example\n",
    "\n",
    "print(\"\\nConverting 'labels' column to plain Python lists of floats...\")\n",
    "ds_tok = ds_tok.map(convert_labels_to_list, batched=False, with_indices=True, desc=\"Converting labels to list\")\n",
    "print(\"'labels' column conversion complete.\")\n",
    "print(f\"Sample dataset features AFTER label conversion: {ds_tok.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dde3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: After conversion, ds_tok[0]['labels'] type: <class 'torch.Tensor'>\n",
      "DEBUG: After conversion, ds_tok[0]['labels'] value: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1.])\n",
      "\n",
      "Verification of labels column type complete (expecting torch.Tensor).\n"
     ]
    }
   ],
   "source": [
    "# Verify the conversion for example 0\n",
    "if len(ds_tok) > 0:\n",
    "    first_example_labels_after_conversion = ds_tok[0]['labels']\n",
    "    print(f\"DEBUG: After conversion, ds_tok[0]['labels'] type: {type(first_example_labels_after_conversion)}\")\n",
    "    print(f\"DEBUG: After conversion, ds_tok[0]['labels'] value: {first_example_labels_after_conversion}\")\n",
    "\n",
    "    # *** MODIFICATION HERE ***\n",
    "    # Change the type check to expect torch.Tensor, which is what datasets is producing.\n",
    "    if not isinstance(first_example_labels_after_conversion, torch.Tensor):\n",
    "        # If it's not a torch.Tensor, then there's still an unexpected type.\n",
    "        raise TypeError(f\"CRITICAL ERROR: ds_tok[0]['labels'] is NOT a torch.Tensor after conversion: \"\n",
    "                        f\"Type={type(first_example_labels_after_conversion)}\")\n",
    "    else:\n",
    "        # Optional: Verify the dtype of the tensor\n",
    "        if first_example_labels_after_conversion.dtype != torch.float32:\n",
    "            print(f\"WARNING: ds_tok_sample[0]['labels'] is a Tensor but its dtype is {first_example_labels_after_conversion.dtype}, not float32. Consider casting if needed downstream.\")\n",
    "\n",
    "print(\"\\nVerification of labels column type complete (expecting torch.Tensor).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe30be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Integrity Check for Sequence Columns ---\n",
      "Checking example 0/2329158\n",
      "Checking example 50000/2329158\n",
      "Checking example 100000/2329158\n",
      "Checking example 150000/2329158\n",
      "Checking example 200000/2329158\n",
      "Checking example 250000/2329158\n",
      "Checking example 300000/2329158\n",
      "Checking example 350000/2329158\n",
      "Checking example 400000/2329158\n",
      "Checking example 450000/2329158\n",
      "Checking example 500000/2329158\n",
      "Checking example 550000/2329158\n",
      "Checking example 600000/2329158\n",
      "Checking example 650000/2329158\n",
      "Checking example 700000/2329158\n",
      "Checking example 750000/2329158\n",
      "Checking example 800000/2329158\n",
      "Checking example 850000/2329158\n",
      "Checking example 900000/2329158\n",
      "Checking example 950000/2329158\n",
      "Checking example 1000000/2329158\n",
      "Checking example 1050000/2329158\n",
      "Checking example 1100000/2329158\n",
      "Checking example 1150000/2329158\n",
      "Checking example 1200000/2329158\n",
      "Checking example 1250000/2329158\n",
      "Checking example 1300000/2329158\n",
      "Checking example 1350000/2329158\n",
      "Checking example 1400000/2329158\n",
      "Checking example 1450000/2329158\n",
      "Checking example 1500000/2329158\n",
      "Checking example 1550000/2329158\n",
      "Checking example 1600000/2329158\n",
      "Checking example 1650000/2329158\n",
      "Checking example 1700000/2329158\n",
      "Checking example 1750000/2329158\n",
      "Checking example 1800000/2329158\n",
      "Checking example 1850000/2329158\n",
      "Checking example 1900000/2329158\n",
      "Checking example 1950000/2329158\n",
      "Checking example 2000000/2329158\n",
      "Checking example 2050000/2329158\n",
      "Checking example 2100000/2329158\n",
      "Checking example 2150000/2329158\n",
      "Checking example 2200000/2329158\n",
      "Checking example 2250000/2329158\n",
      "Checking example 2300000/2329158\n",
      "\n",
      "--- Data integrity check passed. No problematic scalar or 0-d array entries found. ---\n"
     ]
    }
   ],
   "source": [
    "# If using a sample dataset, change the number of the line ' if i % 50000 == 0' to 1000\n",
    "# --- Data Integrity Check for Sequence Columns (Keep this for robustness) ---\n",
    "print(\"\\n--- Starting Data Integrity Check for Sequence Columns ---\")\n",
    "problem_found = False\n",
    "problematic_cols = ['labels', 'input_ids', 'attention_mask']\n",
    "\n",
    "for i, example in enumerate(ds_tok):\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"Checking example {i}/{len(ds_tok)}\")\n",
    "\n",
    "    for col in problematic_cols:\n",
    "        if col not in example:\n",
    "            print(f\"ERROR: Column '{col}' missing from example {i}\")\n",
    "            problem_found = True\n",
    "            continue\n",
    "\n",
    "        item = example[col]\n",
    "        if col == 'labels':\n",
    "            # Allow for both list and torch.Tensor, as datasets might re-tensorize\n",
    "            if not isinstance(item, (list, torch.Tensor)):\n",
    "                print(f\"ERROR: Example {i}, column '{col}': Expected a list or torch.Tensor, but found type {type(item)} with value {item}\")\n",
    "                problem_found_in_initial_data = True\n",
    "            elif isinstance(item, torch.Tensor):\n",
    "                if item.ndim == 0:\n",
    "                    print(f\"ERROR: Example {i}, column '{col}': Found a 0-dimensional torch.Tensor. Value: {item}\")\n",
    "                    problem_found_in_initial_data = True\n",
    "                elif item.dtype not in [torch.float32, torch.float64]: # Check for float dtype\n",
    "                    print(f\"WARNING: Example {i}, column '{col}': torch.Tensor has dtype {item.dtype}, expected float. Value: {item}\")\n",
    "                elif item.shape[-1] != 40: # Assuming 40 is the expected length\n",
    "                    print(f\"WARNING: Example {i}, column '{col}': torch.Tensor length is {item.shape[-1]}, expected 40. Value: {item}\")\n",
    "            elif isinstance(item, list):\n",
    "                if not all(isinstance(x, (float, int)) for x in item):\n",
    "                    print(f\"ERROR: Example {i}, column '{col}': List contains non-numeric elements. Value: {item}\")\n",
    "                    problem_found_in_initial_data = True\n",
    "                elif len(item) == 0:\n",
    "                    print(f\"WARNING: Example {i}, column '{col}': Found an empty list. Value: {item}\")\n",
    "                elif len(item) != 40:\n",
    "                    print(f\"WARNING: Example {i}, column '{col}': List length is {len(item)}, expected 40. Value: {item}\")\n",
    "\n",
    "        else: # For input_ids, attention_mask, etc.\n",
    "            # Keep these checks largely the same, but remember ds_tok_sample.features\n",
    "            # indicates 'input_ids' and 'attention_mask' are Sequence(Value(dtype='int32/int8')).\n",
    "            # Datasets will likely keep these as lists or numpy arrays unless set_format(\"torch\") is used.\n",
    "            if not isinstance(item, (list, np.ndarray, torch.Tensor)): # Add torch.Tensor just in case\n",
    "                print(f\"ERROR: Example {i}, column '{col}': Expected a list, array, or tensor, but found scalar type {type(item)} with value {item}\")\n",
    "                problem_found_in_initial_data = True\n",
    "            elif isinstance(item, np.ndarray) and item.ndim == 0:\n",
    "                print(f\"ERROR: Example {i}, column '{col}': Found a 0-dimensional NumPy array. Value: {item}\")\n",
    "                problem_found_in_initial_data = True\n",
    "            elif isinstance(item, torch.Tensor) and item.ndim == 0:\n",
    "                 print(f\"ERROR: Example {i}, column '{col}': Found a 0-dimensional torch.Tensor. Value: {item}\")\n",
    "                 problem_found_in_initial_data = True\n",
    "            elif isinstance(item, (list, np.ndarray, torch.Tensor)) and (len(item) == 0 if isinstance(item, list) else item.size == 0):\n",
    "                print(f\"WARNING: Example {i}, column '{col}': Found an empty list/array/tensor. Value: {item}\")\n",
    "            elif isinstance(item, list) and not all(isinstance(x, (float, int)) for x in item): # assuming int types for ids/mask\n",
    "                print(f\"ERROR: Example {i}, column '{col}': List contains non-numeric elements. Value: {item}\")\n",
    "            # For tensors/arrays, you might want to check dtype too\n",
    "            elif isinstance(item, (np.ndarray, torch.Tensor)) and item.dtype not in [np.int32, np.int8, torch.int32, torch.int8, torch.long]:\n",
    "                print(f\"WARNING: Example {i}, column '{col}': Array/Tensor has unexpected dtype {item.dtype}. Value: {item}\")\n",
    "        \n",
    "if problem_found:\n",
    "    print(\"\\n--- Data integrity issues found and attempted to fix. Please review warnings/errors. ---\")\n",
    "else:\n",
    "    print(\"\\n--- Data integrity check passed. No problematic scalar or 0-d array entries found. ---\")\n",
    "\n",
    "# --- END Data Integrity Check ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "916f0be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pipeline with model: /data4/mmendieta/models/labse_finetuned_twitter_all_labels/legendary-eon-1/epoch_19/ on device: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmendieta/transformers/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the pipeline with the model of choice\n",
    "print(f\"Initializing pipeline with model: {args.path_to_model_on_disk} on device: {args.cuda_device}\")\n",
    "violence_pipe = pipeline(model=args.path_to_model_on_disk,\n",
    "                         task=\"text-classification\", # This line helps with e5. For the other models is not necessary\n",
    "                         device=args.cuda_device,\n",
    "                         framework=\"pt\",\n",
    "                         return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4bd42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8fbb214aa6435496fa032f2e321514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2329158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code takes approximately 1h 45 min to run on the full test dataset\n",
    "# Perform Inference\n",
    "preds = []\n",
    "\n",
    "for i, outputs in enumerate(tqdm(violence_pipe(KeyDataset(ds_tok, \"text\"), batch_size=args.batch_size,\n",
    "                                              truncation=True),\n",
    "                                 total=len(ds_tok))):\n",
    "    preds.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807b282a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'post1geo10', 'score': 0.5103994607925415},\n",
       " {'label': 'post1geo20', 'score': 0.5083104968070984},\n",
       " {'label': 'post1geo30', 'score': 0.505756139755249},\n",
       " {'label': 'post1geo50', 'score': 0.4982481896877289},\n",
       " {'label': 'post1geo70', 'score': 0.6435137987136841},\n",
       " {'label': 'post2geo10', 'score': 0.48857489228248596},\n",
       " {'label': 'post2geo20', 'score': 0.48798850178718567},\n",
       " {'label': 'post2geo30', 'score': 0.4826910197734833},\n",
       " {'label': 'post2geo50', 'score': 0.4746701419353485},\n",
       " {'label': 'post2geo70', 'score': 0.6336734890937805},\n",
       " {'label': 'post3geo10', 'score': 0.47698694467544556},\n",
       " {'label': 'post3geo20', 'score': 0.47777342796325684},\n",
       " {'label': 'post3geo30', 'score': 0.46957069635391235},\n",
       " {'label': 'post3geo50', 'score': 0.45954760909080505},\n",
       " {'label': 'post3geo70', 'score': 0.6274043321609497},\n",
       " {'label': 'post7geo10', 'score': 0.4379730522632599},\n",
       " {'label': 'post7geo20', 'score': 0.43846064805984497},\n",
       " {'label': 'post7geo30', 'score': 0.42034977674484253},\n",
       " {'label': 'post7geo50', 'score': 0.39889204502105713},\n",
       " {'label': 'post7geo70', 'score': 0.6028570532798767},\n",
       " {'label': 'pre1geo10', 'score': 0.5066482424736023},\n",
       " {'label': 'pre1geo20', 'score': 0.5058010816574097},\n",
       " {'label': 'pre1geo30', 'score': 0.5055418610572815},\n",
       " {'label': 'pre1geo50', 'score': 0.496917188167572},\n",
       " {'label': 'pre1geo70', 'score': 0.6394330263137817},\n",
       " {'label': 'pre2geo10', 'score': 0.4947558343410492},\n",
       " {'label': 'pre2geo20', 'score': 0.4973897933959961},\n",
       " {'label': 'pre2geo30', 'score': 0.4956568777561188},\n",
       " {'label': 'pre2geo50', 'score': 0.48467132449150085},\n",
       " {'label': 'pre2geo70', 'score': 0.6344323754310608},\n",
       " {'label': 'pre3geo10', 'score': 0.48113366961479187},\n",
       " {'label': 'pre3geo20', 'score': 0.483814001083374},\n",
       " {'label': 'pre3geo30', 'score': 0.4806273281574249},\n",
       " {'label': 'pre3geo50', 'score': 0.4689052402973175},\n",
       " {'label': 'pre3geo70', 'score': 0.6243163347244263},\n",
       " {'label': 'pre7geo10', 'score': 0.42665374279022217},\n",
       " {'label': 'pre7geo20', 'score': 0.4309331178665161},\n",
       " {'label': 'pre7geo30', 'score': 0.42783260345458984},\n",
       " {'label': 'pre7geo50', 'score': 0.4003334641456604},\n",
       " {'label': 'pre7geo70', 'score': 0.5790326595306396}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "becca792",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for pred in preds:\n",
    "    scores = {item['label']: item['score'] for item in pred}\n",
    "    processed_data.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8b33b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'post1geo10': 0.5103994607925415,\n",
       " 'post1geo20': 0.5083104968070984,\n",
       " 'post1geo30': 0.505756139755249,\n",
       " 'post1geo50': 0.4982481896877289,\n",
       " 'post1geo70': 0.6435137987136841,\n",
       " 'post2geo10': 0.48857489228248596,\n",
       " 'post2geo20': 0.48798850178718567,\n",
       " 'post2geo30': 0.4826910197734833,\n",
       " 'post2geo50': 0.4746701419353485,\n",
       " 'post2geo70': 0.6336734890937805,\n",
       " 'post3geo10': 0.47698694467544556,\n",
       " 'post3geo20': 0.47777342796325684,\n",
       " 'post3geo30': 0.46957069635391235,\n",
       " 'post3geo50': 0.45954760909080505,\n",
       " 'post3geo70': 0.6274043321609497,\n",
       " 'post7geo10': 0.4379730522632599,\n",
       " 'post7geo20': 0.43846064805984497,\n",
       " 'post7geo30': 0.42034977674484253,\n",
       " 'post7geo50': 0.39889204502105713,\n",
       " 'post7geo70': 0.6028570532798767,\n",
       " 'pre1geo10': 0.5066482424736023,\n",
       " 'pre1geo20': 0.5058010816574097,\n",
       " 'pre1geo30': 0.5055418610572815,\n",
       " 'pre1geo50': 0.496917188167572,\n",
       " 'pre1geo70': 0.6394330263137817,\n",
       " 'pre2geo10': 0.4947558343410492,\n",
       " 'pre2geo20': 0.4973897933959961,\n",
       " 'pre2geo30': 0.4956568777561188,\n",
       " 'pre2geo50': 0.48467132449150085,\n",
       " 'pre2geo70': 0.6344323754310608,\n",
       " 'pre3geo10': 0.48113366961479187,\n",
       " 'pre3geo20': 0.483814001083374,\n",
       " 'pre3geo30': 0.4806273281574249,\n",
       " 'pre3geo50': 0.4689052402973175,\n",
       " 'pre3geo70': 0.6243163347244263,\n",
       " 'pre7geo10': 0.42665374279022217,\n",
       " 'pre7geo20': 0.4309331178665161,\n",
       " 'pre7geo30': 0.42783260345458984,\n",
       " 'pre7geo50': 0.4003334641456604,\n",
       " 'pre7geo70': 0.5790326595306396}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa03f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline output label to 'pred_' column name mapping created:\n",
      "{'post1geo10': 'pred_post1geo10', 'post1geo20': 'pred_post1geo20', 'post1geo30': 'pred_post1geo30', 'post1geo50': 'pred_post1geo50', 'post1geo70': 'pred_post1geo70', 'post2geo10': 'pred_post2geo10', 'post2geo20': 'pred_post2geo20', 'post2geo30': 'pred_post2geo30', 'post2geo50': 'pred_post2geo50', 'post2geo70': 'pred_post2geo70', 'post3geo10': 'pred_post3geo10', 'post3geo20': 'pred_post3geo20', 'post3geo30': 'pred_post3geo30', 'post3geo50': 'pred_post3geo50', 'post3geo70': 'pred_post3geo70', 'post7geo10': 'pred_post7geo10', 'post7geo20': 'pred_post7geo20', 'post7geo30': 'pred_post7geo30', 'post7geo50': 'pred_post7geo50', 'post7geo70': 'pred_post7geo70', 'pre1geo10': 'pred_pre1geo10', 'pre1geo20': 'pred_pre1geo20', 'pre1geo30': 'pred_pre1geo30', 'pre1geo50': 'pred_pre1geo50', 'pre1geo70': 'pred_pre1geo70', 'pre2geo10': 'pred_pre2geo10', 'pre2geo20': 'pred_pre2geo20', 'pre2geo30': 'pred_pre2geo30', 'pre2geo50': 'pred_pre2geo50', 'pre2geo70': 'pred_pre2geo70', 'pre3geo10': 'pred_pre3geo10', 'pre3geo20': 'pred_pre3geo20', 'pre3geo30': 'pred_pre3geo30', 'pre3geo50': 'pred_pre3geo50', 'pre3geo70': 'pred_pre3geo70', 'pre7geo10': 'pred_pre7geo10', 'pre7geo20': 'pred_pre7geo20', 'pre7geo30': 'pred_pre7geo30', 'pre7geo50': 'pred_pre7geo50', 'pre7geo70': 'pred_pre7geo70'}\n"
     ]
    }
   ],
   "source": [
    "# --- CRITICAL: Aligning Pipeline Output Labels to Your True Labels ---\n",
    "# Get the generic labels (e.g., 'LABEL_0', 'LABEL_1', ...) from the pipeline's output.\n",
    "if processed_data:\n",
    "    if isinstance(processed_data[0], list):\n",
    "        # Expected format: [{'label': 'post1geo10', 'score': 0.9}, {'label': 'pre2geo20', 'score': 0.1}]\n",
    "        pipeline_output_labels = [item['label'] for item in processed_data[0]]\n",
    "    elif isinstance(processed_data[0], dict):\n",
    "        # Less common for return_all_scores=True but possible: {'post1geo10': 0.9, 'pre2geo20': 0.1}\n",
    "        pipeline_output_labels = list(processed_data[0].keys())\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected format for processed_data[0]: {type(processed_data[0])}\")\n",
    "\n",
    "    # Create a mapping from pipeline label (string) to its index in your predefined `labels` list\n",
    "    # This helps sort them consistently even if pipeline output order varies.\n",
    "    pipeline_label_to_id = {label_name: i for i, label_name in enumerate(labels)}\n",
    "\n",
    "    # Sort the pipeline output labels based on your predefined order\n",
    "    # Filter out any pipeline labels that are not in your `labels` list to avoid KeyError\n",
    "    pipeline_output_labels_ordered = sorted(\n",
    "        [lbl for lbl in pipeline_output_labels if lbl in pipeline_label_to_id],\n",
    "        key=lambda lbl: pipeline_label_to_id[lbl]\n",
    "    )\n",
    "\n",
    "    if not pipeline_output_labels_ordered:\n",
    "        print(\"WARNING: No pipeline output labels matched your predefined 'labels' list. Check model's output labels.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"No predictions were processed. Ensure your inference code runs correctly.\")\n",
    "\n",
    "\n",
    "if len(pipeline_output_labels_ordered) != len(labels):\n",
    "    print(f\"WARNING: Mismatch in label count! Pipeline outputs {len(pipeline_output_labels_ordered)} labels, \"\n",
    "          f\"but you have {len(labels)} true labels defined. \"\n",
    "          \"This might indicate some labels were not predicted or are named differently.\")\n",
    "    print(f\"Predefined labels: {labels}\")\n",
    "    print(f\"Pipeline discovered labels (ordered): {pipeline_output_labels_ordered}\")\n",
    "\n",
    "\n",
    "pipeline_label_to_pred_col_name_map = {}\n",
    "for actual_label_name in pipeline_output_labels_ordered:\n",
    "    # Ensure only labels that are in your original `labels` list are mapped\n",
    "    if actual_label_name in labels:\n",
    "        pipeline_label_to_pred_col_name_map[actual_label_name] = f\"pred_{actual_label_name}\"\n",
    "    else:\n",
    "        print(f\"WARNING: Skipping pipeline label '{actual_label_name}' as it's not in your predefined 'labels' list.\")\n",
    "\n",
    "print(\"\\nPipeline output label to 'pred_' column name mapping created:\")\n",
    "print(pipeline_label_to_pred_col_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ec0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Append prediction columns to the Dataset ---\n",
    "def add_predictions_to_example_batched(examples, indices): \n",
    "    # 'examples' is a dictionary like {'text': [...], 'input_ids': [...], ...}\n",
    "    # 'indices' is a list of indices for the current batch\n",
    "    batch_size = len(indices)\n",
    "    new_columns_data = {col_name: [] for col_name in pipeline_label_to_pred_col_name_map.values()}\n",
    "\n",
    "    for i, global_idx in enumerate(indices):\n",
    "        # Access predictions for the current example within the batch\n",
    "        raw_prediction_output_for_example = processed_data[global_idx]\n",
    "\n",
    "        # Debugging for a specific example (e.g., global_idx 999)\n",
    "        # Remember: 'labels' here would be examples['labels'][i] for the current example in the batch\n",
    "        if global_idx == 999 and i < batch_size: # Ensure index is within current batch\n",
    "            print(f\"\\n--- DEBUGGING EXAMPLE {global_idx} in add_predictions_to_example_batched ---\")\n",
    "            print(f\"Labels column content (for this example in batch): {examples['labels'][i]}\")\n",
    "            print(f\"Labels column type (for this example in batch): {type(examples['labels'][i])}\")\n",
    "            if isinstance(examples['labels'][i], np.ndarray):\n",
    "                print(f\"Labels column shape (for this example in batch): {examples['labels'][i].shape}\")\n",
    "            print(f\"--- END DEBUGGING EXAMPLE {global_idx} ---\")\n",
    "\n",
    "\n",
    "        for original_label_str, pred_col_name in pipeline_label_to_pred_col_name_map.items():\n",
    "            score = raw_prediction_output_for_example.get(original_label_str, 0.0)\n",
    "\n",
    "            if not isinstance(score, (float, int, np.number)):\n",
    "                raise ValueError(f\"CRITICAL ERROR: Example {global_idx}, Column '{pred_col_name}': \"\n",
    "                                 f\"Score is not a recognized numeric type! Type={type(score)}, Value={score}\")\n",
    "\n",
    "            if isinstance(score, np.number):\n",
    "                score = score.item()\n",
    "\n",
    "            new_columns_data[pred_col_name].append(float(score))\n",
    "\n",
    "    # Return a dictionary where keys are new column names and values are lists of scores for the batch\n",
    "    return new_columns_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32349472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defined final features for dataset after adding prediction columns:\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: Define final_dataset_features (TOP-LEVEL SCRIPT CODE) ---\n",
    "final_dataset_features = ds_tok.features.copy()\n",
    "for pred_col_name in pipeline_label_to_pred_col_name_map.values():\n",
    "    final_dataset_features[pred_col_name] = Value(\"float32\")\n",
    "\n",
    "print(\"\\nDefined final features for dataset after adding prediction columns:\")\n",
    "print(final_dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd16bdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032fbbfc4554a9e99acfade7dab68d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2329158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUGGING EXAMPLE 999 in add_predictions_to_example_batched ---\n",
      "Labels column content (for this example in batch): tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1.])\n",
      "Labels column type (for this example in batch): <class 'torch.Tensor'>\n",
      "--- END DEBUGGING EXAMPLE 999 ---\n",
      "Prediction columns added to the dataset.\n",
      "{'tweetid': Value(dtype='string', id=None), 'geo_x': Value(dtype='float64', id=None), 'geo_y': Value(dtype='float64', id=None), 'lang': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'pred_post1geo10': Value(dtype='float32', id=None), 'pred_post1geo20': Value(dtype='float32', id=None), 'pred_post1geo30': Value(dtype='float32', id=None), 'pred_post1geo50': Value(dtype='float32', id=None), 'pred_post1geo70': Value(dtype='float32', id=None), 'pred_post2geo10': Value(dtype='float32', id=None), 'pred_post2geo20': Value(dtype='float32', id=None), 'pred_post2geo30': Value(dtype='float32', id=None), 'pred_post2geo50': Value(dtype='float32', id=None), 'pred_post2geo70': Value(dtype='float32', id=None), 'pred_post3geo10': Value(dtype='float32', id=None), 'pred_post3geo20': Value(dtype='float32', id=None), 'pred_post3geo30': Value(dtype='float32', id=None), 'pred_post3geo50': Value(dtype='float32', id=None), 'pred_post3geo70': Value(dtype='float32', id=None), 'pred_post7geo10': Value(dtype='float32', id=None), 'pred_post7geo20': Value(dtype='float32', id=None), 'pred_post7geo30': Value(dtype='float32', id=None), 'pred_post7geo50': Value(dtype='float32', id=None), 'pred_post7geo70': Value(dtype='float32', id=None), 'pred_pre1geo10': Value(dtype='float32', id=None), 'pred_pre1geo20': Value(dtype='float32', id=None), 'pred_pre1geo30': Value(dtype='float32', id=None), 'pred_pre1geo50': Value(dtype='float32', id=None), 'pred_pre1geo70': Value(dtype='float32', id=None), 'pred_pre2geo10': Value(dtype='float32', id=None), 'pred_pre2geo20': Value(dtype='float32', id=None), 'pred_pre2geo30': Value(dtype='float32', id=None), 'pred_pre2geo50': Value(dtype='float32', id=None), 'pred_pre2geo70': Value(dtype='float32', id=None), 'pred_pre3geo10': Value(dtype='float32', id=None), 'pred_pre3geo20': Value(dtype='float32', id=None), 'pred_pre3geo30': Value(dtype='float32', id=None), 'pred_pre3geo50': Value(dtype='float32', id=None), 'pred_pre3geo70': Value(dtype='float32', id=None), 'pred_pre7geo10': Value(dtype='float32', id=None), 'pred_pre7geo20': Value(dtype='float32', id=None), 'pred_pre7geo30': Value(dtype='float32', id=None), 'pred_pre7geo50': Value(dtype='float32', id=None), 'pred_pre7geo70': Value(dtype='float32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 3: Call .map using the batched function (TOP-LEVEL SCRIPT CODE) ---\n",
    "ds_with_predictions = ds_tok.map(\n",
    "    add_predictions_to_example_batched, # This is the *function object* passed to map\n",
    "    with_indices=True,\n",
    "    batched=True,                       # Critical for performance\n",
    "    batch_size=args.batch_size,                    # Adjust as needed\n",
    "    features=final_dataset_features,\n",
    "    # Remove writer_batch_size=1\n",
    ")\n",
    "\n",
    "print(\"Prediction columns added to the dataset.\")\n",
    "print(ds_with_predictions.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff389ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the resulting dataset (first row):\n",
      "text: talking abt my case \n",
      "labels: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1.])\n",
      "pred_post1geo10: 0.5103994607925415\n",
      "pred_post1geo20: 0.5083104968070984\n",
      "pred_post1geo30: 0.505756139755249\n",
      "pred_post1geo50: 0.4982481896877289\n",
      "pred_post1geo70: 0.6435137987136841\n",
      "pred_post2geo10: 0.48857489228248596\n",
      "pred_post2geo20: 0.48798850178718567\n",
      "pred_post2geo30: 0.4826910197734833\n",
      "pred_post2geo50: 0.4746701419353485\n",
      "pred_post2geo70: 0.6336734890937805\n",
      "pred_post3geo10: 0.47698694467544556\n",
      "pred_post3geo20: 0.47777342796325684\n",
      "pred_post3geo30: 0.46957069635391235\n",
      "pred_post3geo50: 0.45954760909080505\n",
      "pred_post3geo70: 0.6274043321609497\n",
      "pred_post7geo10: 0.4379730522632599\n",
      "pred_post7geo20: 0.43846064805984497\n",
      "pred_post7geo30: 0.42034977674484253\n",
      "pred_post7geo50: 0.39889204502105713\n",
      "pred_post7geo70: 0.6028570532798767\n",
      "pred_pre1geo10: 0.5066482424736023\n",
      "pred_pre1geo20: 0.5058010816574097\n",
      "pred_pre1geo30: 0.5055418610572815\n",
      "pred_pre1geo50: 0.496917188167572\n",
      "pred_pre1geo70: 0.6394330263137817\n",
      "pred_pre2geo10: 0.4947558343410492\n",
      "pred_pre2geo20: 0.4973897933959961\n",
      "pred_pre2geo30: 0.4956568777561188\n",
      "pred_pre2geo50: 0.48467132449150085\n",
      "pred_pre2geo70: 0.6344323754310608\n",
      "pred_pre3geo10: 0.48113366961479187\n",
      "pred_pre3geo20: 0.483814001083374\n",
      "pred_pre3geo30: 0.4806273281574249\n",
      "pred_pre3geo50: 0.4689052402973175\n",
      "pred_pre3geo70: 0.6243163347244263\n",
      "pred_pre7geo10: 0.42665374279022217\n",
      "pred_pre7geo20: 0.4309331178665161\n",
      "pred_pre7geo30: 0.42783260345458984\n",
      "pred_pre7geo50: 0.4003334641456604\n",
      "pred_pre7geo70: 0.5790326595306396\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4: Sample of the resulting dataset (TOP-LEVEL SCRIPT CODE) ---\n",
    "print(\"\\nSample of the resulting dataset (first row):\")\n",
    "if len(ds_with_predictions) > 0:\n",
    "    first_sample = ds_with_predictions[0]\n",
    "    for key, value in first_sample.items():\n",
    "        if key in ['labels', 'text'] or key.startswith('pred_'):\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"Dataset is empty after mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7335f274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f4cde1239345d7a359e2ea07f8bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/2329158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with predictions successfully saved to: /data4/mmendieta/data/geo_corpus.0.0.1_tok_test_ds_labse_inference_results\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 5: Save the dataset ---\n",
    "try:\n",
    "    ds_with_predictions.save_to_disk(args.fout_inference)\n",
    "    print(f\"Dataset with predictions successfully saved to: {args.fout_inference}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to save dataset with predictions to disk: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e33b386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweetid', 'geo_x', 'geo_y', 'lang', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'pred_post1geo10', 'pred_post1geo20', 'pred_post1geo30', 'pred_post1geo50', 'pred_post1geo70', 'pred_post2geo10', 'pred_post2geo20', 'pred_post2geo30', 'pred_post2geo50', 'pred_post2geo70', 'pred_post3geo10', 'pred_post3geo20', 'pred_post3geo30', 'pred_post3geo50', 'pred_post3geo70', 'pred_post7geo10', 'pred_post7geo20', 'pred_post7geo30', 'pred_post7geo50', 'pred_post7geo70', 'pred_pre1geo10', 'pred_pre1geo20', 'pred_pre1geo30', 'pred_pre1geo50', 'pred_pre1geo70', 'pred_pre2geo10', 'pred_pre2geo20', 'pred_pre2geo30', 'pred_pre2geo50', 'pred_pre2geo70', 'pred_pre3geo10', 'pred_pre3geo20', 'pred_pre3geo30', 'pred_pre3geo50', 'pred_pre3geo70', 'pred_pre7geo10', 'pred_pre7geo20', 'pred_pre7geo30', 'pred_pre7geo50', 'pred_pre7geo70'],\n",
       "    num_rows: 2329158\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a3a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
